{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16kUDPh56pcQI-WEwf6s0ghuNjtXJd2mT",
      "authorship_tag": "ABX9TyOmb6ut+pwlorg5ehTrvwUN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahopman/CTP/blob/main/CTPSetUp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = \"/content/drive/MyDrive/MS_DataScience/DS595/CTP\""
      ],
      "metadata": {
        "id": "Soz6UqMD63wn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip materials"
      ],
      "metadata": {
        "id": "hssWZNdc_Lb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## FIGURE OUT HOW TO DOWNLOAD materials.zip ##"
      ],
      "metadata": {
        "id": "bKYYJlWVMHE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrVoGpSt0yys",
        "outputId": "0a094d8a-f3d1-44a8-8646-c785f07868fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials.zip\n",
            "   creating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/\n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/sec2label.json  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/split2ids.json  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/pmc_contents.json  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/pmcid2picoid.json  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/secname2sec.json  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/sec2count.json  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/prompt_labels.json  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/CTP/evidence_integration/materials/prompt_info.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip {local_path}/evidence_integration/materials.zip -d {local_path}/evidence_integration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Evidence Integration"
      ],
      "metadata": {
        "id": "4_1OHN3a_Hkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'\n",
        "\n",
        "import json\n",
        "\n",
        "def generate(picoids, split_path):\n",
        "\toutput = []\n",
        "\tfor picoid in picoids:\n",
        "\t\tif prompt_info[picoid]['label'] != 'invalid prompt':\n",
        "\t\t\toutput.append({})\n",
        "\t\t\toutput[-1]['picoid'] = picoid\n",
        "\t\t\toutput[-1]['pmcid'] = prompt_info[picoid]['PMCID']\n",
        "\t\t\toutput[-1]['i_text'] = prompt_info[picoid]['I']\n",
        "\t\t\toutput[-1]['c_text'] = prompt_info[picoid]['C']\n",
        "\t\t\toutput[-1]['o_text'] = prompt_info[picoid]['O']\n",
        "\t\t\toutput[-1]['label'] = result2label[prompt_info[picoid]['label']]\n",
        "\n",
        "\t\t\tpassage = ''\n",
        "\t\t\tif str(prompt_info[picoid]['PMCID']) in pmcid2content:\n",
        "\t\t\t\tcontent = pmcid2content[str(prompt_info[picoid]['PMCID'])]\n",
        "\t\t\t\tfor secname, text in content:\n",
        "\t\t\t\t\tif secname[:len('ABSTRACT')] != 'ABSTRACT': continue\n",
        "\t\t\t\t\tif sec2label[secname2sec[secname]] == '1':\n",
        "\t\t\t\t\t\tpassage += text\n",
        "\n",
        "\t\t\toutput[-1]['passage'] = passage\n",
        "\n",
        "\twith open(split_path, 'w') as f:\n",
        "\t\tjson.dump(output, f, indent=4)"
      ],
      "metadata": {
        "id": "9DHauIXd6mRc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'\n",
        "\n",
        "result2label = {'significantly decreased': 0,\\\n",
        "\t\t\t\t'no significant difference': 1,\\\n",
        "\t\t\t\t'significantly increased': 2}\n",
        "\n",
        "prompt_info = json.load(open(local_path + '/evidence_integration/materials/prompt_info.json'))\n",
        "split2ids = json.load(open(local_path + '/evidence_integration/materials/split2ids.json'))\n",
        "pmcid2picoid = json.load(open(local_path + '/evidence_integration/materials/pmcid2picoid.json'))\n",
        "pmcid2content = json.load(open(local_path + '/evidence_integration/materials/pmc_contents.json'))\n",
        "secname2sec = json.load(open(local_path + '/evidence_integration/materials/secname2sec.json'))\n",
        "sec2label = json.load(open(local_path + '/evidence_integration/materials/sec2label.json'))\n",
        "\n",
        "for split, ids in split2ids.items():\n",
        "    picoids = []\n",
        "\n",
        "    for pmcid in ids:\n",
        "\t    pmcid = str(pmcid)\n",
        "        if pmcid in pmcid2picoid:\n",
        "\t\t    picoids += pmcid2picoid[pmcid]\n",
        "\n",
        "    split_path = f\"{local_path}/evidence_integration/{split}.json\"\n",
        "    generate(picoids, split_path)"
      ],
      "metadata": {
        "id": "vYlliamz6FPk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index dataset"
      ],
      "metadata": {
        "id": "1-NWmrPN_Nzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'\n",
        "\n",
        "splits = ['train', 'validation', 'test']\n",
        "\n",
        "for split in splits:\n",
        "    split_path = local_path + '/evidence_integration/' + split + '.json'\n",
        "    ori_data = json.load(open(split_path))\n",
        "    picos = []\n",
        "    ctxs = []\n",
        "    pmcid2ctxid = {}\n",
        "\n",
        "\n",
        "    for entry in ori_data:\n",
        "        pico = {k: entry[k] for k in ['i_text', 'c_text', 'o_text', 'label']}\n",
        "        pmcid = entry['pmcid']\n",
        "\n",
        "        if pmcid not in pmcid2ctxid:\n",
        "            pmcid2ctxid[pmcid] = len(ctxs)\n",
        "            ctx = {'ctx_id': pmcid2ctxid[pmcid], 'passage': entry['passage']}\n",
        "            ctxs.append(ctx)\n",
        "\n",
        "        pico['ctx_id'] = pmcid2ctxid[pmcid]\n",
        "        picos.append(pico)\n",
        "\n",
        "    with open(f\"{local_path}/evidence_integration/indexed_{split}_picos.json\", 'w') as f:\n",
        "        json.dump(picos, f, indent=4)\n",
        "    with open(f\"{local_path}/evidence_integration/indexed_{split}_ctxs.json\", 'w') as f:\n",
        "        json.dump(ctxs, f, indent=4)"
      ],
      "metadata": {
        "id": "rO3cRPkM_GBL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and unzip PubMed baseline splits"
      ],
      "metadata": {
        "id": "ATP79-1mBQjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Mia Hopman'\n",
        "\n",
        "import os\n",
        "from ftplib import FTP\n",
        "import gzip\n",
        "\n",
        "def download_and_extract_gz_files(ftp_server, ftp_path, destination_path):\n",
        "    if not os.path.exists(destination_path):\n",
        "        os.makedirs(destination_path)\n",
        "\n",
        "    ftp = FTP(ftp_server)\n",
        "    ftp.login()\n",
        "    ftp.cwd(ftp_path)\n",
        "\n",
        "    file_list = ftp.nlst()\n",
        "\n",
        "    for file_name in file_list:\n",
        "        if os.path.exists(os.path.join(destination_path, file_name)):\n",
        "            print(f'{file_name} already exists. Skipping download.')\n",
        "        else:\n",
        "            if file_name.endswith('.gz'):\n",
        "                local_file_path = os.path.join(destination_path, file_name)\n",
        "                with open(local_file_path, 'wb') as f:\n",
        "                    ftp.retrbinary(f'RETR {file_name}', f.write)\n",
        "\n",
        "                with gzip.open(local_file_path, 'rb') as gz_file:\n",
        "                    uncompressed_file_path = os.path.splitext(local_file_path)[0]\n",
        "                    with open(uncompressed_file_path, 'wb') as uncompressed_file:\n",
        "                        uncompressed_file.write(gz_file.read())\n",
        "\n",
        "                print(f'Downloaded and extracted {file_name}')\n",
        "\n",
        "    ftp.quit()"
      ],
      "metadata": {
        "id": "L9xyzIViBVKZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Mia Hopman'\n",
        "\n",
        "ftp_server = 'ftp.ncbi.nlm.nih.gov'\n",
        "ftp_path = '/pubmed/baseline/'\n",
        "destination_path = f\"{local_path}/pretraining_dataset/pubmed_baseline\"\n",
        "\n",
        "download_and_extract_gz_files(ftp_server, ftp_path, destination_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO7ktXz3BQNp",
        "outputId": "4eb28c0d-0c46-43c4-8b99-b82138fab34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pubmed24n0002.xml.gz already exists. Skipping download.\n",
            "pubmed24n0004.xml.gz already exists. Skipping download.\n",
            "pubmed24n0003.xml.gz already exists. Skipping download.\n",
            "pubmed24n0001.xml.gz already exists. Skipping download.\n",
            "pubmed24n0006.xml.gz already exists. Skipping download.\n",
            "pubmed24n0008.xml.gz already exists. Skipping download.\n",
            "pubmed24n0005.xml.gz already exists. Skipping download.\n",
            "pubmed24n0009.xml.gz already exists. Skipping download.\n",
            "pubmed24n0007.xml.gz already exists. Skipping download.\n",
            "pubmed24n0010.xml.gz already exists. Skipping download.\n",
            "pubmed24n0013.xml.gz already exists. Skipping download.\n",
            "pubmed24n0014.xml.gz already exists. Skipping download.\n",
            "pubmed24n0011.xml.gz already exists. Skipping download.\n",
            "pubmed24n0012.xml.gz already exists. Skipping download.\n",
            "pubmed24n0015.xml.gz already exists. Skipping download.\n",
            "pubmed24n0016.xml.gz already exists. Skipping download.\n",
            "pubmed24n0018.xml.gz already exists. Skipping download.\n",
            "pubmed24n0017.xml.gz already exists. Skipping download.\n",
            "pubmed24n0021.xml.gz already exists. Skipping download.\n",
            "pubmed24n0020.xml.gz already exists. Skipping download.\n",
            "pubmed24n0019.xml.gz already exists. Skipping download.\n",
            "pubmed24n0022.xml.gz already exists. Skipping download.\n",
            "pubmed24n0023.xml.gz already exists. Skipping download.\n",
            "pubmed24n0025.xml.gz already exists. Skipping download.\n",
            "pubmed24n0024.xml.gz already exists. Skipping download.\n",
            "pubmed24n0026.xml.gz already exists. Skipping download.\n",
            "pubmed24n0079.xml.gz already exists. Skipping download.\n",
            "pubmed24n0028.xml.gz already exists. Skipping download.\n",
            "pubmed24n0027.xml.gz already exists. Skipping download.\n",
            "pubmed24n0029.xml.gz already exists. Skipping download.\n",
            "pubmed24n0030.xml.gz already exists. Skipping download.\n",
            "pubmed24n0032.xml.gz already exists. Skipping download.\n",
            "pubmed24n0031.xml.gz already exists. Skipping download.\n",
            "pubmed24n0033.xml.gz already exists. Skipping download.\n",
            "pubmed24n0034.xml.gz already exists. Skipping download.\n",
            "pubmed24n0035.xml.gz already exists. Skipping download.\n",
            "pubmed24n0036.xml.gz already exists. Skipping download.\n",
            "pubmed24n0037.xml.gz already exists. Skipping download.\n",
            "pubmed24n0038.xml.gz already exists. Skipping download.\n",
            "pubmed24n0039.xml.gz already exists. Skipping download.\n",
            "pubmed24n0040.xml.gz already exists. Skipping download.\n",
            "pubmed24n0041.xml.gz already exists. Skipping download.\n",
            "pubmed24n0042.xml.gz already exists. Skipping download.\n",
            "pubmed24n0043.xml.gz already exists. Skipping download.\n",
            "pubmed24n0044.xml.gz already exists. Skipping download.\n",
            "pubmed24n0045.xml.gz already exists. Skipping download.\n",
            "pubmed24n0046.xml.gz already exists. Skipping download.\n",
            "pubmed24n0047.xml.gz already exists. Skipping download.\n",
            "pubmed24n0048.xml.gz already exists. Skipping download.\n",
            "pubmed24n0049.xml.gz already exists. Skipping download.\n",
            "pubmed24n0050.xml.gz already exists. Skipping download.\n",
            "pubmed24n0051.xml.gz already exists. Skipping download.\n",
            "pubmed24n0052.xml.gz already exists. Skipping download.\n",
            "pubmed24n0053.xml.gz already exists. Skipping download.\n",
            "pubmed24n0054.xml.gz already exists. Skipping download.\n",
            "pubmed24n0055.xml.gz already exists. Skipping download.\n",
            "pubmed24n0056.xml.gz already exists. Skipping download.\n",
            "pubmed24n0057.xml.gz already exists. Skipping download.\n",
            "pubmed24n0058.xml.gz already exists. Skipping download.\n",
            "pubmed24n0059.xml.gz already exists. Skipping download.\n",
            "pubmed24n0060.xml.gz already exists. Skipping download.\n",
            "pubmed24n0061.xml.gz already exists. Skipping download.\n",
            "pubmed24n0062.xml.gz already exists. Skipping download.\n",
            "pubmed24n0063.xml.gz already exists. Skipping download.\n",
            "pubmed24n0064.xml.gz already exists. Skipping download.\n",
            "pubmed24n0065.xml.gz already exists. Skipping download.\n",
            "pubmed24n0066.xml.gz already exists. Skipping download.\n",
            "pubmed24n0067.xml.gz already exists. Skipping download.\n",
            "pubmed24n0068.xml.gz already exists. Skipping download.\n",
            "pubmed24n0069.xml.gz already exists. Skipping download.\n",
            "pubmed24n0070.xml.gz already exists. Skipping download.\n",
            "pubmed24n0071.xml.gz already exists. Skipping download.\n",
            "pubmed24n0072.xml.gz already exists. Skipping download.\n",
            "pubmed24n0073.xml.gz already exists. Skipping download.\n",
            "pubmed24n0074.xml.gz already exists. Skipping download.\n",
            "pubmed24n0075.xml.gz already exists. Skipping download.\n",
            "pubmed24n0076.xml.gz already exists. Skipping download.\n",
            "pubmed24n0077.xml.gz already exists. Skipping download.\n",
            "pubmed24n0078.xml.gz already exists. Skipping download.\n",
            "pubmed24n0132.xml.gz already exists. Skipping download.\n",
            "pubmed24n0080.xml.gz already exists. Skipping download.\n",
            "pubmed24n0081.xml.gz already exists. Skipping download.\n",
            "pubmed24n0082.xml.gz already exists. Skipping download.\n",
            "pubmed24n0083.xml.gz already exists. Skipping download.\n",
            "pubmed24n0085.xml.gz already exists. Skipping download.\n",
            "pubmed24n0084.xml.gz already exists. Skipping download.\n",
            "pubmed24n0086.xml.gz already exists. Skipping download.\n",
            "pubmed24n0087.xml.gz already exists. Skipping download.\n",
            "pubmed24n0088.xml.gz already exists. Skipping download.\n",
            "pubmed24n0090.xml.gz already exists. Skipping download.\n",
            "pubmed24n0089.xml.gz already exists. Skipping download.\n",
            "pubmed24n0091.xml.gz already exists. Skipping download.\n",
            "pubmed24n0092.xml.gz already exists. Skipping download.\n",
            "pubmed24n0093.xml.gz already exists. Skipping download.\n",
            "pubmed24n0094.xml.gz already exists. Skipping download.\n",
            "pubmed24n0095.xml.gz already exists. Skipping download.\n",
            "pubmed24n0096.xml.gz already exists. Skipping download.\n",
            "pubmed24n0098.xml.gz already exists. Skipping download.\n",
            "pubmed24n0097.xml.gz already exists. Skipping download.\n",
            "pubmed24n0100.xml.gz already exists. Skipping download.\n",
            "pubmed24n0101.xml.gz already exists. Skipping download.\n",
            "pubmed24n0099.xml.gz already exists. Skipping download.\n",
            "pubmed24n0102.xml.gz already exists. Skipping download.\n",
            "pubmed24n0103.xml.gz already exists. Skipping download.\n",
            "pubmed24n0106.xml.gz already exists. Skipping download.\n",
            "pubmed24n0105.xml.gz already exists. Skipping download.\n",
            "pubmed24n0104.xml.gz already exists. Skipping download.\n",
            "pubmed24n0107.xml.gz already exists. Skipping download.\n",
            "pubmed24n0108.xml.gz already exists. Skipping download.\n",
            "pubmed24n0109.xml.gz already exists. Skipping download.\n",
            "pubmed24n0110.xml.gz already exists. Skipping download.\n",
            "pubmed24n0111.xml.gz already exists. Skipping download.\n",
            "pubmed24n0112.xml.gz already exists. Skipping download.\n",
            "pubmed24n0114.xml.gz already exists. Skipping download.\n",
            "pubmed24n0113.xml.gz already exists. Skipping download.\n",
            "pubmed24n0115.xml.gz already exists. Skipping download.\n",
            "pubmed24n0116.xml.gz already exists. Skipping download.\n",
            "pubmed24n0117.xml.gz already exists. Skipping download.\n",
            "pubmed24n0118.xml.gz already exists. Skipping download.\n",
            "pubmed24n0119.xml.gz already exists. Skipping download.\n",
            "pubmed24n0120.xml.gz already exists. Skipping download.\n",
            "pubmed24n0121.xml.gz already exists. Skipping download.\n",
            "pubmed24n0122.xml.gz already exists. Skipping download.\n",
            "pubmed24n0123.xml.gz already exists. Skipping download.\n",
            "pubmed24n0124.xml.gz already exists. Skipping download.\n",
            "pubmed24n0125.xml.gz already exists. Skipping download.\n",
            "pubmed24n0126.xml.gz already exists. Skipping download.\n",
            "pubmed24n0127.xml.gz already exists. Skipping download.\n",
            "pubmed24n0128.xml.gz already exists. Skipping download.\n",
            "pubmed24n0129.xml.gz already exists. Skipping download.\n",
            "pubmed24n0130.xml.gz already exists. Skipping download.\n",
            "pubmed24n0131.xml.gz already exists. Skipping download.\n",
            "pubmed24n0185.xml.gz already exists. Skipping download.\n",
            "pubmed24n0133.xml.gz already exists. Skipping download.\n",
            "pubmed24n0134.xml.gz already exists. Skipping download.\n",
            "pubmed24n0135.xml.gz already exists. Skipping download.\n",
            "pubmed24n0136.xml.gz already exists. Skipping download.\n",
            "pubmed24n0137.xml.gz already exists. Skipping download.\n",
            "pubmed24n0138.xml.gz already exists. Skipping download.\n",
            "pubmed24n0139.xml.gz already exists. Skipping download.\n",
            "pubmed24n0140.xml.gz already exists. Skipping download.\n",
            "pubmed24n0141.xml.gz already exists. Skipping download.\n",
            "pubmed24n0142.xml.gz already exists. Skipping download.\n",
            "pubmed24n0143.xml.gz already exists. Skipping download.\n",
            "pubmed24n0144.xml.gz already exists. Skipping download.\n",
            "pubmed24n0145.xml.gz already exists. Skipping download.\n",
            "pubmed24n0146.xml.gz already exists. Skipping download.\n",
            "pubmed24n0147.xml.gz already exists. Skipping download.\n",
            "pubmed24n0148.xml.gz already exists. Skipping download.\n",
            "pubmed24n0149.xml.gz already exists. Skipping download.\n",
            "pubmed24n0150.xml.gz already exists. Skipping download.\n",
            "pubmed24n0151.xml.gz already exists. Skipping download.\n",
            "pubmed24n0152.xml.gz already exists. Skipping download.\n",
            "pubmed24n0153.xml.gz already exists. Skipping download.\n",
            "pubmed24n0154.xml.gz already exists. Skipping download.\n",
            "pubmed24n0155.xml.gz already exists. Skipping download.\n",
            "pubmed24n0156.xml.gz already exists. Skipping download.\n",
            "pubmed24n0157.xml.gz already exists. Skipping download.\n",
            "pubmed24n0158.xml.gz already exists. Skipping download.\n",
            "pubmed24n0159.xml.gz already exists. Skipping download.\n",
            "pubmed24n0160.xml.gz already exists. Skipping download.\n",
            "pubmed24n0161.xml.gz already exists. Skipping download.\n",
            "pubmed24n0162.xml.gz already exists. Skipping download.\n",
            "pubmed24n0163.xml.gz already exists. Skipping download.\n",
            "pubmed24n0164.xml.gz already exists. Skipping download.\n",
            "pubmed24n0165.xml.gz already exists. Skipping download.\n",
            "pubmed24n0166.xml.gz already exists. Skipping download.\n",
            "pubmed24n0167.xml.gz already exists. Skipping download.\n",
            "pubmed24n0168.xml.gz already exists. Skipping download.\n",
            "pubmed24n0169.xml.gz already exists. Skipping download.\n",
            "pubmed24n0170.xml.gz already exists. Skipping download.\n",
            "pubmed24n0171.xml.gz already exists. Skipping download.\n",
            "pubmed24n0172.xml.gz already exists. Skipping download.\n",
            "pubmed24n0173.xml.gz already exists. Skipping download.\n",
            "pubmed24n0174.xml.gz already exists. Skipping download.\n",
            "pubmed24n0175.xml.gz already exists. Skipping download.\n",
            "pubmed24n0176.xml.gz already exists. Skipping download.\n",
            "pubmed24n0177.xml.gz already exists. Skipping download.\n",
            "pubmed24n0178.xml.gz already exists. Skipping download.\n",
            "pubmed24n0179.xml.gz already exists. Skipping download.\n",
            "pubmed24n0180.xml.gz already exists. Skipping download.\n",
            "pubmed24n0181.xml.gz already exists. Skipping download.\n",
            "pubmed24n0182.xml.gz already exists. Skipping download.\n",
            "pubmed24n0183.xml.gz already exists. Skipping download.\n",
            "pubmed24n0184.xml.gz already exists. Skipping download.\n",
            "pubmed24n0238.xml.gz already exists. Skipping download.\n",
            "pubmed24n0186.xml.gz already exists. Skipping download.\n",
            "pubmed24n0187.xml.gz already exists. Skipping download.\n",
            "pubmed24n0188.xml.gz already exists. Skipping download.\n",
            "pubmed24n0189.xml.gz already exists. Skipping download.\n",
            "pubmed24n0190.xml.gz already exists. Skipping download.\n",
            "pubmed24n0191.xml.gz already exists. Skipping download.\n",
            "pubmed24n0192.xml.gz already exists. Skipping download.\n",
            "pubmed24n0193.xml.gz already exists. Skipping download.\n",
            "pubmed24n0194.xml.gz already exists. Skipping download.\n",
            "pubmed24n0195.xml.gz already exists. Skipping download.\n",
            "pubmed24n0196.xml.gz already exists. Skipping download.\n",
            "pubmed24n0197.xml.gz already exists. Skipping download.\n",
            "pubmed24n0198.xml.gz already exists. Skipping download.\n",
            "pubmed24n0199.xml.gz already exists. Skipping download.\n",
            "pubmed24n0200.xml.gz already exists. Skipping download.\n",
            "pubmed24n0201.xml.gz already exists. Skipping download.\n",
            "pubmed24n0202.xml.gz already exists. Skipping download.\n",
            "pubmed24n0203.xml.gz already exists. Skipping download.\n",
            "pubmed24n0204.xml.gz already exists. Skipping download.\n",
            "pubmed24n0205.xml.gz already exists. Skipping download.\n",
            "pubmed24n0206.xml.gz already exists. Skipping download.\n",
            "pubmed24n0207.xml.gz already exists. Skipping download.\n",
            "pubmed24n0208.xml.gz already exists. Skipping download.\n",
            "pubmed24n0209.xml.gz already exists. Skipping download.\n",
            "pubmed24n0210.xml.gz already exists. Skipping download.\n",
            "pubmed24n0211.xml.gz already exists. Skipping download.\n",
            "pubmed24n0212.xml.gz already exists. Skipping download.\n",
            "pubmed24n0213.xml.gz already exists. Skipping download.\n",
            "pubmed24n0215.xml.gz already exists. Skipping download.\n",
            "pubmed24n0214.xml.gz already exists. Skipping download.\n",
            "pubmed24n0216.xml.gz already exists. Skipping download.\n",
            "pubmed24n0217.xml.gz already exists. Skipping download.\n",
            "pubmed24n0218.xml.gz already exists. Skipping download.\n",
            "pubmed24n0219.xml.gz already exists. Skipping download.\n",
            "pubmed24n0220.xml.gz already exists. Skipping download.\n",
            "pubmed24n0221.xml.gz already exists. Skipping download.\n",
            "pubmed24n0222.xml.gz already exists. Skipping download.\n",
            "pubmed24n0223.xml.gz already exists. Skipping download.\n",
            "pubmed24n0224.xml.gz already exists. Skipping download.\n",
            "pubmed24n0225.xml.gz already exists. Skipping download.\n",
            "pubmed24n0226.xml.gz already exists. Skipping download.\n",
            "pubmed24n0227.xml.gz already exists. Skipping download.\n",
            "pubmed24n0228.xml.gz already exists. Skipping download.\n",
            "pubmed24n0229.xml.gz already exists. Skipping download.\n",
            "pubmed24n0230.xml.gz already exists. Skipping download.\n",
            "pubmed24n0231.xml.gz already exists. Skipping download.\n",
            "pubmed24n0232.xml.gz already exists. Skipping download.\n",
            "pubmed24n0233.xml.gz already exists. Skipping download.\n",
            "pubmed24n0234.xml.gz already exists. Skipping download.\n",
            "pubmed24n0235.xml.gz already exists. Skipping download.\n",
            "pubmed24n0236.xml.gz already exists. Skipping download.\n",
            "pubmed24n0237.xml.gz already exists. Skipping download.\n",
            "pubmed24n0291.xml.gz already exists. Skipping download.\n",
            "pubmed24n0239.xml.gz already exists. Skipping download.\n",
            "pubmed24n0240.xml.gz already exists. Skipping download.\n",
            "pubmed24n0241.xml.gz already exists. Skipping download.\n",
            "pubmed24n0242.xml.gz already exists. Skipping download.\n",
            "pubmed24n0243.xml.gz already exists. Skipping download.\n",
            "pubmed24n0244.xml.gz already exists. Skipping download.\n",
            "pubmed24n0245.xml.gz already exists. Skipping download.\n",
            "pubmed24n0246.xml.gz already exists. Skipping download.\n",
            "pubmed24n0247.xml.gz already exists. Skipping download.\n",
            "pubmed24n0248.xml.gz already exists. Skipping download.\n",
            "pubmed24n0249.xml.gz already exists. Skipping download.\n",
            "pubmed24n0250.xml.gz already exists. Skipping download.\n",
            "pubmed24n0251.xml.gz already exists. Skipping download.\n",
            "pubmed24n0252.xml.gz already exists. Skipping download.\n",
            "pubmed24n0253.xml.gz already exists. Skipping download.\n",
            "pubmed24n0254.xml.gz already exists. Skipping download.\n",
            "pubmed24n0255.xml.gz already exists. Skipping download.\n",
            "pubmed24n0256.xml.gz already exists. Skipping download.\n",
            "pubmed24n0257.xml.gz already exists. Skipping download.\n",
            "pubmed24n0258.xml.gz already exists. Skipping download.\n",
            "pubmed24n0259.xml.gz already exists. Skipping download.\n",
            "pubmed24n0260.xml.gz already exists. Skipping download.\n",
            "pubmed24n0261.xml.gz already exists. Skipping download.\n",
            "pubmed24n0262.xml.gz already exists. Skipping download.\n",
            "pubmed24n0263.xml.gz already exists. Skipping download.\n",
            "pubmed24n0264.xml.gz already exists. Skipping download.\n",
            "pubmed24n0265.xml.gz already exists. Skipping download.\n",
            "pubmed24n0266.xml.gz already exists. Skipping download.\n",
            "pubmed24n0267.xml.gz already exists. Skipping download.\n",
            "pubmed24n0268.xml.gz already exists. Skipping download.\n",
            "pubmed24n0269.xml.gz already exists. Skipping download.\n",
            "pubmed24n0270.xml.gz already exists. Skipping download.\n",
            "pubmed24n0271.xml.gz already exists. Skipping download.\n",
            "pubmed24n0272.xml.gz already exists. Skipping download.\n",
            "pubmed24n0273.xml.gz already exists. Skipping download.\n",
            "pubmed24n0274.xml.gz already exists. Skipping download.\n",
            "pubmed24n0275.xml.gz already exists. Skipping download.\n",
            "pubmed24n0276.xml.gz already exists. Skipping download.\n",
            "pubmed24n0277.xml.gz already exists. Skipping download.\n",
            "pubmed24n0278.xml.gz already exists. Skipping download.\n",
            "pubmed24n0279.xml.gz already exists. Skipping download.\n",
            "pubmed24n0281.xml.gz already exists. Skipping download.\n",
            "pubmed24n0280.xml.gz already exists. Skipping download.\n",
            "pubmed24n0282.xml.gz already exists. Skipping download.\n",
            "pubmed24n0283.xml.gz already exists. Skipping download.\n",
            "pubmed24n0284.xml.gz already exists. Skipping download.\n",
            "pubmed24n0285.xml.gz already exists. Skipping download.\n",
            "pubmed24n0286.xml.gz already exists. Skipping download.\n",
            "pubmed24n0287.xml.gz already exists. Skipping download.\n",
            "pubmed24n0289.xml.gz already exists. Skipping download.\n",
            "pubmed24n0288.xml.gz already exists. Skipping download.\n",
            "pubmed24n0290.xml.gz already exists. Skipping download.\n",
            "pubmed24n0344.xml.gz already exists. Skipping download.\n",
            "pubmed24n0292.xml.gz already exists. Skipping download.\n",
            "pubmed24n0293.xml.gz already exists. Skipping download.\n",
            "pubmed24n0294.xml.gz already exists. Skipping download.\n",
            "pubmed24n0295.xml.gz already exists. Skipping download.\n",
            "pubmed24n0297.xml.gz already exists. Skipping download.\n",
            "pubmed24n0296.xml.gz already exists. Skipping download.\n",
            "pubmed24n0298.xml.gz already exists. Skipping download.\n",
            "pubmed24n0299.xml.gz already exists. Skipping download.\n",
            "pubmed24n0301.xml.gz already exists. Skipping download.\n",
            "pubmed24n0300.xml.gz already exists. Skipping download.\n",
            "pubmed24n0302.xml.gz already exists. Skipping download.\n",
            "pubmed24n0303.xml.gz already exists. Skipping download.\n",
            "pubmed24n0305.xml.gz already exists. Skipping download.\n",
            "pubmed24n0304.xml.gz already exists. Skipping download.\n",
            "pubmed24n0306.xml.gz already exists. Skipping download.\n",
            "pubmed24n0307.xml.gz already exists. Skipping download.\n",
            "pubmed24n0308.xml.gz already exists. Skipping download.\n",
            "pubmed24n0309.xml.gz already exists. Skipping download.\n",
            "pubmed24n0310.xml.gz already exists. Skipping download.\n",
            "pubmed24n0311.xml.gz already exists. Skipping download.\n",
            "pubmed24n0312.xml.gz already exists. Skipping download.\n",
            "pubmed24n0313.xml.gz already exists. Skipping download.\n",
            "pubmed24n0314.xml.gz already exists. Skipping download.\n",
            "pubmed24n0315.xml.gz already exists. Skipping download.\n",
            "pubmed24n0316.xml.gz already exists. Skipping download.\n",
            "pubmed24n0317.xml.gz already exists. Skipping download.\n",
            "pubmed24n0318.xml.gz already exists. Skipping download.\n",
            "pubmed24n0319.xml.gz already exists. Skipping download.\n",
            "pubmed24n0320.xml.gz already exists. Skipping download.\n",
            "pubmed24n0321.xml.gz already exists. Skipping download.\n",
            "pubmed24n0322.xml.gz already exists. Skipping download.\n",
            "pubmed24n0323.xml.gz already exists. Skipping download.\n",
            "pubmed24n0324.xml.gz already exists. Skipping download.\n",
            "pubmed24n0326.xml.gz already exists. Skipping download.\n",
            "pubmed24n0325.xml.gz already exists. Skipping download.\n",
            "pubmed24n0327.xml.gz already exists. Skipping download.\n",
            "pubmed24n0328.xml.gz already exists. Skipping download.\n",
            "pubmed24n0330.xml.gz already exists. Skipping download.\n",
            "pubmed24n0329.xml.gz already exists. Skipping download.\n",
            "pubmed24n0331.xml.gz already exists. Skipping download.\n",
            "pubmed24n0332.xml.gz already exists. Skipping download.\n",
            "pubmed24n0333.xml.gz already exists. Skipping download.\n",
            "pubmed24n0334.xml.gz already exists. Skipping download.\n",
            "pubmed24n0335.xml.gz already exists. Skipping download.\n",
            "pubmed24n0336.xml.gz already exists. Skipping download.\n",
            "pubmed24n0337.xml.gz already exists. Skipping download.\n",
            "pubmed24n0338.xml.gz already exists. Skipping download.\n",
            "pubmed24n0341.xml.gz already exists. Skipping download.\n",
            "pubmed24n0340.xml.gz already exists. Skipping download.\n",
            "pubmed24n0339.xml.gz already exists. Skipping download.\n",
            "pubmed24n0342.xml.gz already exists. Skipping download.\n",
            "pubmed24n0343.xml.gz already exists. Skipping download.\n",
            "pubmed24n0397.xml.gz already exists. Skipping download.\n",
            "pubmed24n0345.xml.gz already exists. Skipping download.\n",
            "pubmed24n0346.xml.gz already exists. Skipping download.\n",
            "pubmed24n0347.xml.gz already exists. Skipping download.\n",
            "pubmed24n0348.xml.gz already exists. Skipping download.\n",
            "pubmed24n0349.xml.gz already exists. Skipping download.\n",
            "pubmed24n0351.xml.gz already exists. Skipping download.\n",
            "pubmed24n0352.xml.gz already exists. Skipping download.\n",
            "pubmed24n0350.xml.gz already exists. Skipping download.\n",
            "pubmed24n0353.xml.gz already exists. Skipping download.\n",
            "pubmed24n0355.xml.gz already exists. Skipping download.\n",
            "pubmed24n0354.xml.gz already exists. Skipping download.\n",
            "pubmed24n0356.xml.gz already exists. Skipping download.\n",
            "pubmed24n0357.xml.gz already exists. Skipping download.\n",
            "pubmed24n0359.xml.gz already exists. Skipping download.\n",
            "pubmed24n0358.xml.gz already exists. Skipping download.\n",
            "pubmed24n0360.xml.gz already exists. Skipping download.\n",
            "pubmed24n0361.xml.gz already exists. Skipping download.\n",
            "pubmed24n0362.xml.gz already exists. Skipping download.\n",
            "pubmed24n0365.xml.gz already exists. Skipping download.\n",
            "pubmed24n0363.xml.gz already exists. Skipping download.\n",
            "pubmed24n0364.xml.gz already exists. Skipping download.\n",
            "pubmed24n0366.xml.gz already exists. Skipping download.\n",
            "pubmed24n0368.xml.gz already exists. Skipping download.\n",
            "pubmed24n0367.xml.gz already exists. Skipping download.\n",
            "pubmed24n0369.xml.gz already exists. Skipping download.\n",
            "pubmed24n0370.xml.gz already exists. Skipping download.\n",
            "pubmed24n0371.xml.gz already exists. Skipping download.\n",
            "pubmed24n0372.xml.gz already exists. Skipping download.\n",
            "pubmed24n0373.xml.gz already exists. Skipping download.\n",
            "pubmed24n0374.xml.gz already exists. Skipping download.\n",
            "pubmed24n0375.xml.gz already exists. Skipping download.\n",
            "pubmed24n0376.xml.gz already exists. Skipping download.\n",
            "pubmed24n0377.xml.gz already exists. Skipping download.\n",
            "pubmed24n0378.xml.gz already exists. Skipping download.\n",
            "pubmed24n0379.xml.gz already exists. Skipping download.\n",
            "pubmed24n0380.xml.gz already exists. Skipping download.\n",
            "pubmed24n0381.xml.gz already exists. Skipping download.\n",
            "pubmed24n0382.xml.gz already exists. Skipping download.\n",
            "pubmed24n0384.xml.gz already exists. Skipping download.\n",
            "pubmed24n0383.xml.gz already exists. Skipping download.\n",
            "pubmed24n0385.xml.gz already exists. Skipping download.\n",
            "pubmed24n0386.xml.gz already exists. Skipping download.\n",
            "pubmed24n0387.xml.gz already exists. Skipping download.\n",
            "pubmed24n0388.xml.gz already exists. Skipping download.\n",
            "pubmed24n0389.xml.gz already exists. Skipping download.\n",
            "pubmed24n0390.xml.gz already exists. Skipping download.\n",
            "pubmed24n0391.xml.gz already exists. Skipping download.\n",
            "pubmed24n0392.xml.gz already exists. Skipping download.\n",
            "pubmed24n0393.xml.gz already exists. Skipping download.\n",
            "pubmed24n0394.xml.gz already exists. Skipping download.\n",
            "pubmed24n0395.xml.gz already exists. Skipping download.\n",
            "pubmed24n0396.xml.gz already exists. Skipping download.\n",
            "pubmed24n0450.xml.gz already exists. Skipping download.\n",
            "pubmed24n0398.xml.gz already exists. Skipping download.\n",
            "pubmed24n0399.xml.gz already exists. Skipping download.\n",
            "pubmed24n0401.xml.gz already exists. Skipping download.\n",
            "pubmed24n0400.xml.gz already exists. Skipping download.\n",
            "pubmed24n0402.xml.gz already exists. Skipping download.\n",
            "pubmed24n0403.xml.gz already exists. Skipping download.\n",
            "pubmed24n0404.xml.gz already exists. Skipping download.\n",
            "pubmed24n0405.xml.gz already exists. Skipping download.\n",
            "pubmed24n0406.xml.gz already exists. Skipping download.\n",
            "pubmed24n0407.xml.gz already exists. Skipping download.\n",
            "pubmed24n0408.xml.gz already exists. Skipping download.\n",
            "pubmed24n0409.xml.gz already exists. Skipping download.\n",
            "pubmed24n0410.xml.gz already exists. Skipping download.\n",
            "pubmed24n0411.xml.gz already exists. Skipping download.\n",
            "pubmed24n0412.xml.gz already exists. Skipping download.\n",
            "pubmed24n0413.xml.gz already exists. Skipping download.\n",
            "pubmed24n0414.xml.gz already exists. Skipping download.\n",
            "pubmed24n0415.xml.gz already exists. Skipping download.\n",
            "pubmed24n0416.xml.gz already exists. Skipping download.\n",
            "pubmed24n0417.xml.gz already exists. Skipping download.\n",
            "pubmed24n0418.xml.gz already exists. Skipping download.\n",
            "pubmed24n0419.xml.gz already exists. Skipping download.\n",
            "pubmed24n0420.xml.gz already exists. Skipping download.\n",
            "pubmed24n0421.xml.gz already exists. Skipping download.\n",
            "pubmed24n0422.xml.gz already exists. Skipping download.\n",
            "pubmed24n0424.xml.gz already exists. Skipping download.\n",
            "pubmed24n0425.xml.gz already exists. Skipping download.\n",
            "pubmed24n0423.xml.gz already exists. Skipping download.\n",
            "pubmed24n0426.xml.gz already exists. Skipping download.\n",
            "pubmed24n0427.xml.gz already exists. Skipping download.\n",
            "pubmed24n0428.xml.gz already exists. Skipping download.\n",
            "pubmed24n0429.xml.gz already exists. Skipping download.\n",
            "pubmed24n0430.xml.gz already exists. Skipping download.\n",
            "pubmed24n0431.xml.gz already exists. Skipping download.\n",
            "pubmed24n0432.xml.gz already exists. Skipping download.\n",
            "pubmed24n0433.xml.gz already exists. Skipping download.\n",
            "pubmed24n0434.xml.gz already exists. Skipping download.\n",
            "pubmed24n0435.xml.gz already exists. Skipping download.\n",
            "pubmed24n0436.xml.gz already exists. Skipping download.\n",
            "pubmed24n0437.xml.gz already exists. Skipping download.\n",
            "pubmed24n0438.xml.gz already exists. Skipping download.\n",
            "pubmed24n0439.xml.gz already exists. Skipping download.\n",
            "pubmed24n0440.xml.gz already exists. Skipping download.\n",
            "pubmed24n0441.xml.gz already exists. Skipping download.\n",
            "pubmed24n0442.xml.gz already exists. Skipping download.\n",
            "pubmed24n0445.xml.gz already exists. Skipping download.\n",
            "pubmed24n0443.xml.gz already exists. Skipping download.\n",
            "pubmed24n0444.xml.gz already exists. Skipping download.\n",
            "pubmed24n0446.xml.gz already exists. Skipping download.\n",
            "pubmed24n0447.xml.gz already exists. Skipping download.\n",
            "pubmed24n0448.xml.gz already exists. Skipping download.\n",
            "pubmed24n0449.xml.gz already exists. Skipping download.\n",
            "pubmed24n0502.xml.gz already exists. Skipping download.\n",
            "pubmed24n0451.xml.gz already exists. Skipping download.\n",
            "pubmed24n0453.xml.gz already exists. Skipping download.\n",
            "pubmed24n0452.xml.gz already exists. Skipping download.\n",
            "pubmed24n0454.xml.gz already exists. Skipping download.\n",
            "pubmed24n0455.xml.gz already exists. Skipping download.\n",
            "pubmed24n0456.xml.gz already exists. Skipping download.\n",
            "pubmed24n0457.xml.gz already exists. Skipping download.\n",
            "pubmed24n0458.xml.gz already exists. Skipping download.\n",
            "pubmed24n0459.xml.gz already exists. Skipping download.\n",
            "pubmed24n0460.xml.gz already exists. Skipping download.\n",
            "pubmed24n0461.xml.gz already exists. Skipping download.\n",
            "pubmed24n0462.xml.gz already exists. Skipping download.\n",
            "pubmed24n0463.xml.gz already exists. Skipping download.\n",
            "pubmed24n0464.xml.gz already exists. Skipping download.\n",
            "pubmed24n0465.xml.gz already exists. Skipping download.\n",
            "pubmed24n0466.xml.gz already exists. Skipping download.\n",
            "pubmed24n0467.xml.gz already exists. Skipping download.\n",
            "pubmed24n0468.xml.gz already exists. Skipping download.\n",
            "pubmed24n0469.xml.gz already exists. Skipping download.\n",
            "pubmed24n0470.xml.gz already exists. Skipping download.\n",
            "pubmed24n0471.xml.gz already exists. Skipping download.\n",
            "pubmed24n0472.xml.gz already exists. Skipping download.\n",
            "pubmed24n0473.xml.gz already exists. Skipping download.\n",
            "pubmed24n0474.xml.gz already exists. Skipping download.\n",
            "pubmed24n0475.xml.gz already exists. Skipping download.\n",
            "pubmed24n0476.xml.gz already exists. Skipping download.\n",
            "pubmed24n0477.xml.gz already exists. Skipping download.\n",
            "pubmed24n0478.xml.gz already exists. Skipping download.\n",
            "pubmed24n0479.xml.gz already exists. Skipping download.\n",
            "pubmed24n0481.xml.gz already exists. Skipping download.\n",
            "pubmed24n0480.xml.gz already exists. Skipping download.\n",
            "pubmed24n0482.xml.gz already exists. Skipping download.\n",
            "pubmed24n0483.xml.gz already exists. Skipping download.\n",
            "pubmed24n0484.xml.gz already exists. Skipping download.\n",
            "pubmed24n0485.xml.gz already exists. Skipping download.\n",
            "pubmed24n0486.xml.gz already exists. Skipping download.\n",
            "pubmed24n0487.xml.gz already exists. Skipping download.\n",
            "pubmed24n0488.xml.gz already exists. Skipping download.\n",
            "pubmed24n0489.xml.gz already exists. Skipping download.\n",
            "pubmed24n0490.xml.gz already exists. Skipping download.\n",
            "pubmed24n0491.xml.gz already exists. Skipping download.\n",
            "pubmed24n0492.xml.gz already exists. Skipping download.\n",
            "pubmed24n0493.xml.gz already exists. Skipping download.\n",
            "pubmed24n0494.xml.gz already exists. Skipping download.\n",
            "pubmed24n0495.xml.gz already exists. Skipping download.\n",
            "pubmed24n0496.xml.gz already exists. Skipping download.\n",
            "pubmed24n0497.xml.gz already exists. Skipping download.\n",
            "pubmed24n0498.xml.gz already exists. Skipping download.\n",
            "pubmed24n0499.xml.gz already exists. Skipping download.\n",
            "pubmed24n0500.xml.gz already exists. Skipping download.\n",
            "pubmed24n0501.xml.gz already exists. Skipping download.\n",
            "pubmed24n0503.xml.gz already exists. Skipping download.\n",
            "pubmed24n0554.xml.gz already exists. Skipping download.\n",
            "pubmed24n0504.xml.gz already exists. Skipping download.\n",
            "pubmed24n0505.xml.gz already exists. Skipping download.\n",
            "pubmed24n0506.xml.gz already exists. Skipping download.\n",
            "pubmed24n0507.xml.gz already exists. Skipping download.\n",
            "pubmed24n0508.xml.gz already exists. Skipping download.\n",
            "pubmed24n0509.xml.gz already exists. Skipping download.\n",
            "pubmed24n0510.xml.gz already exists. Skipping download.\n",
            "pubmed24n0511.xml.gz already exists. Skipping download.\n",
            "pubmed24n0512.xml.gz already exists. Skipping download.\n",
            "pubmed24n0513.xml.gz already exists. Skipping download.\n",
            "pubmed24n0514.xml.gz already exists. Skipping download.\n",
            "pubmed24n0515.xml.gz already exists. Skipping download.\n",
            "pubmed24n0516.xml.gz already exists. Skipping download.\n",
            "pubmed24n0517.xml.gz already exists. Skipping download.\n",
            "pubmed24n0518.xml.gz already exists. Skipping download.\n",
            "pubmed24n0519.xml.gz already exists. Skipping download.\n",
            "pubmed24n0520.xml.gz already exists. Skipping download.\n",
            "pubmed24n0523.xml.gz already exists. Skipping download.\n",
            "pubmed24n0524.xml.gz already exists. Skipping download.\n",
            "pubmed24n0522.xml.gz already exists. Skipping download.\n",
            "pubmed24n0521.xml.gz already exists. Skipping download.\n",
            "pubmed24n0525.xml.gz already exists. Skipping download.\n",
            "pubmed24n0526.xml.gz already exists. Skipping download.\n",
            "pubmed24n0527.xml.gz already exists. Skipping download.\n",
            "pubmed24n0528.xml.gz already exists. Skipping download.\n",
            "pubmed24n0529.xml.gz already exists. Skipping download.\n",
            "pubmed24n0530.xml.gz already exists. Skipping download.\n",
            "pubmed24n0531.xml.gz already exists. Skipping download.\n",
            "pubmed24n0532.xml.gz already exists. Skipping download.\n",
            "pubmed24n0533.xml.gz already exists. Skipping download.\n",
            "pubmed24n0534.xml.gz already exists. Skipping download.\n",
            "pubmed24n0536.xml.gz already exists. Skipping download.\n",
            "pubmed24n0535.xml.gz already exists. Skipping download.\n",
            "pubmed24n0537.xml.gz already exists. Skipping download.\n",
            "pubmed24n0538.xml.gz already exists. Skipping download.\n",
            "pubmed24n0539.xml.gz already exists. Skipping download.\n",
            "pubmed24n0540.xml.gz already exists. Skipping download.\n",
            "pubmed24n0541.xml.gz already exists. Skipping download.\n",
            "pubmed24n0542.xml.gz already exists. Skipping download.\n",
            "pubmed24n0543.xml.gz already exists. Skipping download.\n",
            "pubmed24n0544.xml.gz already exists. Skipping download.\n",
            "pubmed24n0545.xml.gz already exists. Skipping download.\n",
            "pubmed24n0546.xml.gz already exists. Skipping download.\n",
            "pubmed24n0548.xml.gz already exists. Skipping download.\n",
            "pubmed24n0547.xml.gz already exists. Skipping download.\n",
            "pubmed24n0549.xml.gz already exists. Skipping download.\n",
            "pubmed24n0550.xml.gz already exists. Skipping download.\n",
            "pubmed24n0551.xml.gz already exists. Skipping download.\n",
            "pubmed24n0552.xml.gz already exists. Skipping download.\n",
            "pubmed24n0553.xml.gz already exists. Skipping download.\n",
            "pubmed24n0555.xml.gz already exists. Skipping download.\n",
            "pubmed24n0556.xml.gz already exists. Skipping download.\n",
            "pubmed24n0609.xml.gz already exists. Skipping download.\n",
            "pubmed24n0557.xml.gz already exists. Skipping download.\n",
            "pubmed24n0558.xml.gz already exists. Skipping download.\n",
            "pubmed24n0560.xml.gz already exists. Skipping download.\n",
            "pubmed24n0559.xml.gz already exists. Skipping download.\n",
            "pubmed24n0561.xml.gz already exists. Skipping download.\n",
            "pubmed24n0562.xml.gz already exists. Skipping download.\n",
            "pubmed24n0564.xml.gz already exists. Skipping download.\n",
            "pubmed24n0563.xml.gz already exists. Skipping download.\n",
            "pubmed24n0565.xml.gz already exists. Skipping download.\n",
            "pubmed24n0566.xml.gz already exists. Skipping download.\n",
            "pubmed24n0568.xml.gz already exists. Skipping download.\n",
            "pubmed24n0567.xml.gz already exists. Skipping download.\n",
            "pubmed24n0569.xml.gz already exists. Skipping download.\n",
            "pubmed24n0570.xml.gz already exists. Skipping download.\n",
            "pubmed24n0571.xml.gz already exists. Skipping download.\n",
            "pubmed24n0572.xml.gz already exists. Skipping download.\n",
            "pubmed24n0573.xml.gz already exists. Skipping download.\n",
            "pubmed24n0576.xml.gz already exists. Skipping download.\n",
            "pubmed24n0575.xml.gz already exists. Skipping download.\n",
            "pubmed24n0574.xml.gz already exists. Skipping download.\n",
            "pubmed24n0577.xml.gz already exists. Skipping download.\n",
            "pubmed24n0578.xml.gz already exists. Skipping download.\n",
            "pubmed24n0579.xml.gz already exists. Skipping download.\n",
            "pubmed24n0581.xml.gz already exists. Skipping download.\n",
            "pubmed24n0580.xml.gz already exists. Skipping download.\n",
            "pubmed24n0582.xml.gz already exists. Skipping download.\n",
            "pubmed24n0583.xml.gz already exists. Skipping download.\n",
            "pubmed24n0584.xml.gz already exists. Skipping download.\n",
            "pubmed24n0585.xml.gz already exists. Skipping download.\n",
            "pubmed24n0587.xml.gz already exists. Skipping download.\n",
            "pubmed24n0586.xml.gz already exists. Skipping download.\n",
            "pubmed24n0588.xml.gz already exists. Skipping download.\n",
            "pubmed24n0589.xml.gz already exists. Skipping download.\n",
            "pubmed24n0590.xml.gz already exists. Skipping download.\n",
            "pubmed24n0591.xml.gz already exists. Skipping download.\n",
            "pubmed24n0592.xml.gz already exists. Skipping download.\n",
            "pubmed24n0593.xml.gz already exists. Skipping download.\n",
            "pubmed24n0595.xml.gz already exists. Skipping download.\n",
            "pubmed24n0594.xml.gz already exists. Skipping download.\n",
            "pubmed24n0596.xml.gz already exists. Skipping download.\n",
            "pubmed24n0597.xml.gz already exists. Skipping download.\n",
            "pubmed24n0598.xml.gz already exists. Skipping download.\n",
            "pubmed24n0599.xml.gz already exists. Skipping download.\n",
            "pubmed24n0600.xml.gz already exists. Skipping download.\n",
            "pubmed24n0601.xml.gz already exists. Skipping download.\n",
            "pubmed24n0604.xml.gz already exists. Skipping download.\n",
            "pubmed24n0602.xml.gz already exists. Skipping download.\n",
            "pubmed24n0603.xml.gz already exists. Skipping download.\n",
            "pubmed24n0606.xml.gz already exists. Skipping download.\n",
            "pubmed24n0605.xml.gz already exists. Skipping download.\n",
            "pubmed24n0610.xml.gz already exists. Skipping download.\n",
            "pubmed24n0608.xml.gz already exists. Skipping download.\n",
            "pubmed24n0662.xml.gz already exists. Skipping download.\n",
            "pubmed24n0607.xml.gz already exists. Skipping download.\n",
            "pubmed24n0611.xml.gz already exists. Skipping download.\n",
            "pubmed24n0612.xml.gz already exists. Skipping download.\n",
            "pubmed24n0614.xml.gz already exists. Skipping download.\n",
            "pubmed24n0613.xml.gz already exists. Skipping download.\n",
            "pubmed24n0616.xml.gz already exists. Skipping download.\n",
            "pubmed24n0615.xml.gz already exists. Skipping download.\n",
            "pubmed24n0617.xml.gz already exists. Skipping download.\n",
            "pubmed24n0618.xml.gz already exists. Skipping download.\n",
            "pubmed24n0619.xml.gz already exists. Skipping download.\n",
            "pubmed24n0620.xml.gz already exists. Skipping download.\n",
            "pubmed24n0621.xml.gz already exists. Skipping download.\n",
            "pubmed24n0622.xml.gz already exists. Skipping download.\n",
            "pubmed24n0625.xml.gz already exists. Skipping download.\n",
            "pubmed24n0623.xml.gz already exists. Skipping download.\n",
            "pubmed24n0624.xml.gz already exists. Skipping download.\n",
            "pubmed24n0626.xml.gz already exists. Skipping download.\n",
            "pubmed24n0627.xml.gz already exists. Skipping download.\n",
            "pubmed24n0629.xml.gz already exists. Skipping download.\n",
            "pubmed24n0628.xml.gz already exists. Skipping download.\n",
            "pubmed24n0630.xml.gz already exists. Skipping download.\n",
            "pubmed24n0631.xml.gz already exists. Skipping download.\n",
            "pubmed24n0633.xml.gz already exists. Skipping download.\n",
            "pubmed24n0632.xml.gz already exists. Skipping download.\n",
            "pubmed24n0634.xml.gz already exists. Skipping download.\n",
            "pubmed24n0635.xml.gz already exists. Skipping download.\n",
            "pubmed24n0637.xml.gz already exists. Skipping download.\n",
            "pubmed24n0636.xml.gz already exists. Skipping download.\n",
            "pubmed24n0638.xml.gz already exists. Skipping download.\n",
            "pubmed24n0640.xml.gz already exists. Skipping download.\n",
            "pubmed24n0639.xml.gz already exists. Skipping download.\n",
            "pubmed24n0642.xml.gz already exists. Skipping download.\n",
            "pubmed24n0641.xml.gz already exists. Skipping download.\n",
            "pubmed24n0645.xml.gz already exists. Skipping download.\n",
            "pubmed24n0643.xml.gz already exists. Skipping download.\n",
            "pubmed24n0644.xml.gz already exists. Skipping download.\n",
            "pubmed24n0646.xml.gz already exists. Skipping download.\n",
            "pubmed24n0647.xml.gz already exists. Skipping download.\n",
            "pubmed24n0648.xml.gz already exists. Skipping download.\n",
            "pubmed24n0649.xml.gz already exists. Skipping download.\n",
            "pubmed24n0650.xml.gz already exists. Skipping download.\n",
            "pubmed24n0651.xml.gz already exists. Skipping download.\n",
            "pubmed24n0652.xml.gz already exists. Skipping download.\n",
            "pubmed24n0653.xml.gz already exists. Skipping download.\n",
            "pubmed24n0654.xml.gz already exists. Skipping download.\n",
            "pubmed24n0655.xml.gz already exists. Skipping download.\n",
            "pubmed24n0656.xml.gz already exists. Skipping download.\n",
            "pubmed24n0657.xml.gz already exists. Skipping download.\n",
            "pubmed24n0658.xml.gz already exists. Skipping download.\n",
            "pubmed24n0659.xml.gz already exists. Skipping download.\n",
            "pubmed24n0660.xml.gz already exists. Skipping download.\n",
            "pubmed24n0661.xml.gz already exists. Skipping download.\n",
            "pubmed24n0715.xml.gz already exists. Skipping download.\n",
            "pubmed24n0663.xml.gz already exists. Skipping download.\n",
            "pubmed24n0664.xml.gz already exists. Skipping download.\n",
            "pubmed24n0665.xml.gz already exists. Skipping download.\n",
            "pubmed24n0667.xml.gz already exists. Skipping download.\n",
            "pubmed24n0666.xml.gz already exists. Skipping download.\n",
            "pubmed24n0669.xml.gz already exists. Skipping download.\n",
            "pubmed24n0668.xml.gz already exists. Skipping download.\n",
            "pubmed24n0670.xml.gz already exists. Skipping download.\n",
            "pubmed24n0671.xml.gz already exists. Skipping download.\n",
            "pubmed24n0672.xml.gz already exists. Skipping download.\n",
            "pubmed24n0673.xml.gz already exists. Skipping download.\n",
            "pubmed24n0674.xml.gz already exists. Skipping download.\n",
            "pubmed24n0676.xml.gz already exists. Skipping download.\n",
            "pubmed24n0675.xml.gz already exists. Skipping download.\n",
            "pubmed24n0677.xml.gz already exists. Skipping download.\n",
            "pubmed24n0678.xml.gz already exists. Skipping download.\n",
            "pubmed24n0679.xml.gz already exists. Skipping download.\n",
            "pubmed24n0681.xml.gz already exists. Skipping download.\n",
            "pubmed24n0680.xml.gz already exists. Skipping download.\n",
            "pubmed24n0682.xml.gz already exists. Skipping download.\n",
            "pubmed24n0683.xml.gz already exists. Skipping download.\n",
            "pubmed24n0684.xml.gz already exists. Skipping download.\n",
            "pubmed24n0685.xml.gz already exists. Skipping download.\n",
            "pubmed24n0686.xml.gz already exists. Skipping download.\n",
            "pubmed24n0687.xml.gz already exists. Skipping download.\n",
            "pubmed24n0688.xml.gz already exists. Skipping download.\n",
            "pubmed24n0689.xml.gz already exists. Skipping download.\n",
            "pubmed24n0690.xml.gz already exists. Skipping download.\n",
            "pubmed24n0691.xml.gz already exists. Skipping download.\n",
            "pubmed24n0692.xml.gz already exists. Skipping download.\n",
            "pubmed24n0693.xml.gz already exists. Skipping download.\n",
            "pubmed24n0694.xml.gz already exists. Skipping download.\n",
            "pubmed24n0695.xml.gz already exists. Skipping download.\n",
            "pubmed24n0696.xml.gz already exists. Skipping download.\n",
            "pubmed24n0697.xml.gz already exists. Skipping download.\n",
            "pubmed24n0698.xml.gz already exists. Skipping download.\n",
            "pubmed24n0699.xml.gz already exists. Skipping download.\n",
            "pubmed24n0700.xml.gz already exists. Skipping download.\n",
            "pubmed24n0701.xml.gz already exists. Skipping download.\n",
            "pubmed24n0702.xml.gz already exists. Skipping download.\n",
            "pubmed24n0703.xml.gz already exists. Skipping download.\n",
            "pubmed24n0704.xml.gz already exists. Skipping download.\n",
            "pubmed24n0705.xml.gz already exists. Skipping download.\n",
            "pubmed24n0706.xml.gz already exists. Skipping download.\n",
            "pubmed24n0707.xml.gz already exists. Skipping download.\n",
            "pubmed24n0708.xml.gz already exists. Skipping download.\n",
            "pubmed24n0709.xml.gz already exists. Skipping download.\n",
            "pubmed24n0710.xml.gz already exists. Skipping download.\n",
            "pubmed24n0711.xml.gz already exists. Skipping download.\n",
            "pubmed24n0712.xml.gz already exists. Skipping download.\n",
            "pubmed24n0713.xml.gz already exists. Skipping download.\n",
            "pubmed24n0714.xml.gz already exists. Skipping download.\n",
            "pubmed24n0768.xml.gz already exists. Skipping download.\n",
            "pubmed24n0716.xml.gz already exists. Skipping download.\n",
            "pubmed24n0717.xml.gz already exists. Skipping download.\n",
            "pubmed24n0718.xml.gz already exists. Skipping download.\n",
            "pubmed24n0719.xml.gz already exists. Skipping download.\n",
            "pubmed24n0720.xml.gz already exists. Skipping download.\n",
            "pubmed24n0721.xml.gz already exists. Skipping download.\n",
            "pubmed24n0722.xml.gz already exists. Skipping download.\n",
            "pubmed24n0724.xml.gz already exists. Skipping download.\n",
            "pubmed24n0723.xml.gz already exists. Skipping download.\n",
            "pubmed24n0726.xml.gz already exists. Skipping download.\n",
            "pubmed24n0725.xml.gz already exists. Skipping download.\n",
            "pubmed24n0727.xml.gz already exists. Skipping download.\n",
            "pubmed24n0728.xml.gz already exists. Skipping download.\n",
            "pubmed24n0729.xml.gz already exists. Skipping download.\n",
            "pubmed24n0730.xml.gz already exists. Skipping download.\n",
            "pubmed24n0731.xml.gz already exists. Skipping download.\n",
            "pubmed24n0732.xml.gz already exists. Skipping download.\n",
            "pubmed24n0733.xml.gz already exists. Skipping download.\n",
            "pubmed24n0734.xml.gz already exists. Skipping download.\n",
            "pubmed24n0735.xml.gz already exists. Skipping download.\n",
            "pubmed24n0736.xml.gz already exists. Skipping download.\n",
            "pubmed24n0737.xml.gz already exists. Skipping download.\n",
            "pubmed24n0738.xml.gz already exists. Skipping download.\n",
            "pubmed24n0739.xml.gz already exists. Skipping download.\n",
            "pubmed24n0740.xml.gz already exists. Skipping download.\n",
            "pubmed24n0741.xml.gz already exists. Skipping download.\n",
            "pubmed24n0742.xml.gz already exists. Skipping download.\n",
            "pubmed24n0744.xml.gz already exists. Skipping download.\n",
            "pubmed24n0745.xml.gz already exists. Skipping download.\n",
            "pubmed24n0743.xml.gz already exists. Skipping download.\n",
            "pubmed24n0746.xml.gz already exists. Skipping download.\n",
            "pubmed24n0747.xml.gz already exists. Skipping download.\n",
            "pubmed24n0748.xml.gz already exists. Skipping download.\n",
            "pubmed24n0749.xml.gz already exists. Skipping download.\n",
            "pubmed24n0750.xml.gz already exists. Skipping download.\n",
            "pubmed24n0753.xml.gz already exists. Skipping download.\n",
            "pubmed24n0751.xml.gz already exists. Skipping download.\n",
            "pubmed24n0752.xml.gz already exists. Skipping download.\n",
            "pubmed24n0754.xml.gz already exists. Skipping download.\n",
            "pubmed24n0755.xml.gz already exists. Skipping download.\n",
            "pubmed24n0756.xml.gz already exists. Skipping download.\n",
            "pubmed24n0758.xml.gz already exists. Skipping download.\n",
            "pubmed24n0757.xml.gz already exists. Skipping download.\n",
            "pubmed24n0759.xml.gz already exists. Skipping download.\n",
            "pubmed24n0761.xml.gz already exists. Skipping download.\n",
            "pubmed24n0760.xml.gz already exists. Skipping download.\n",
            "pubmed24n0762.xml.gz already exists. Skipping download.\n",
            "pubmed24n0763.xml.gz already exists. Skipping download.\n",
            "pubmed24n0764.xml.gz already exists. Skipping download.\n",
            "pubmed24n0765.xml.gz already exists. Skipping download.\n",
            "pubmed24n0766.xml.gz already exists. Skipping download.\n",
            "pubmed24n0767.xml.gz already exists. Skipping download.\n",
            "pubmed24n0821.xml.gz already exists. Skipping download.\n",
            "pubmed24n0769.xml.gz already exists. Skipping download.\n",
            "pubmed24n0770.xml.gz already exists. Skipping download.\n",
            "pubmed24n0771.xml.gz already exists. Skipping download.\n",
            "pubmed24n0772.xml.gz already exists. Skipping download.\n",
            "pubmed24n0773.xml.gz already exists. Skipping download.\n",
            "pubmed24n0774.xml.gz already exists. Skipping download.\n",
            "pubmed24n0775.xml.gz already exists. Skipping download.\n",
            "pubmed24n0776.xml.gz already exists. Skipping download.\n",
            "pubmed24n0778.xml.gz already exists. Skipping download.\n",
            "pubmed24n0777.xml.gz already exists. Skipping download.\n",
            "pubmed24n0779.xml.gz already exists. Skipping download.\n",
            "pubmed24n0780.xml.gz already exists. Skipping download.\n",
            "pubmed24n0782.xml.gz already exists. Skipping download.\n",
            "pubmed24n0781.xml.gz already exists. Skipping download.\n",
            "pubmed24n0784.xml.gz already exists. Skipping download.\n",
            "pubmed24n0783.xml.gz already exists. Skipping download.\n",
            "pubmed24n0785.xml.gz already exists. Skipping download.\n",
            "Downloaded and extracted pubmed24n0786.xml.gz\n",
            "Downloaded and extracted pubmed24n0787.xml.gz\n",
            "Downloaded and extracted pubmed24n0788.xml.gz\n",
            "Downloaded and extracted pubmed24n0789.xml.gz\n",
            "Downloaded and extracted pubmed24n0790.xml.gz\n",
            "Downloaded and extracted pubmed24n0792.xml.gz\n",
            "Downloaded and extracted pubmed24n0791.xml.gz\n",
            "Downloaded and extracted pubmed24n0793.xml.gz\n",
            "Downloaded and extracted pubmed24n0794.xml.gz\n",
            "Downloaded and extracted pubmed24n0796.xml.gz\n",
            "Downloaded and extracted pubmed24n0795.xml.gz\n",
            "Downloaded and extracted pubmed24n0797.xml.gz\n",
            "Downloaded and extracted pubmed24n0798.xml.gz\n",
            "Downloaded and extracted pubmed24n0799.xml.gz\n",
            "Downloaded and extracted pubmed24n0801.xml.gz\n",
            "Downloaded and extracted pubmed24n0800.xml.gz\n",
            "Downloaded and extracted pubmed24n0802.xml.gz\n",
            "Downloaded and extracted pubmed24n0803.xml.gz\n",
            "Downloaded and extracted pubmed24n0805.xml.gz\n",
            "Downloaded and extracted pubmed24n0804.xml.gz\n",
            "Downloaded and extracted pubmed24n0806.xml.gz\n",
            "Downloaded and extracted pubmed24n0809.xml.gz\n",
            "Downloaded and extracted pubmed24n0807.xml.gz\n",
            "Downloaded and extracted pubmed24n0808.xml.gz\n",
            "Downloaded and extracted pubmed24n0810.xml.gz\n",
            "Downloaded and extracted pubmed24n0812.xml.gz\n",
            "Downloaded and extracted pubmed24n0813.xml.gz\n",
            "Downloaded and extracted pubmed24n0811.xml.gz\n",
            "Downloaded and extracted pubmed24n0814.xml.gz\n",
            "Downloaded and extracted pubmed24n0815.xml.gz\n",
            "Downloaded and extracted pubmed24n0816.xml.gz\n",
            "Downloaded and extracted pubmed24n0817.xml.gz\n",
            "Downloaded and extracted pubmed24n0818.xml.gz\n",
            "Downloaded and extracted pubmed24n0819.xml.gz\n",
            "Downloaded and extracted pubmed24n0820.xml.gz\n",
            "Downloaded and extracted pubmed24n0847.xml.gz\n",
            "Downloaded and extracted pubmed24n0822.xml.gz\n",
            "Downloaded and extracted pubmed24n0824.xml.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess PubMed splits"
      ],
      "metadata": {
        "id": "ziWzaSCfDBjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'\n",
        "\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "for xml_path in glob.glob(f\"{local_path}/pretraining_dataset/pubmed_baseline/pubmed24n*.xml\"):\n",
        "    xml_file = xml_path.split('/')[-1]\n",
        "    print('Processing %s' % xml_path)\n",
        "    output = []\n",
        "\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    for citation in root.iter('MedlineCitation'):\n",
        "        pmid = citation.find('PMID')\n",
        "        if pmid == None:\n",
        "            continue\n",
        "        else:\n",
        "            pmid = pmid.text\n",
        "\n",
        "        texts = []\n",
        "        sec_labels = []\n",
        "\n",
        "        title = citation.find('Article/ArticleTitle')\n",
        "        if title != None:\n",
        "            texts.append(title.text)\n",
        "            sec_labels.append('TITLE')\n",
        "\n",
        "        for info in citation.iter('AbstractText'):\n",
        "            if info.text:\n",
        "                texts.append(info.text)\n",
        "                sec_labels.append(info.get('Label'))\n",
        "\n",
        "        assert len(texts) == len(sec_labels)\n",
        "\n",
        "        output.append({'pmid': pmid,\n",
        "                       'texts': texts,\n",
        "                       'sec_labels': sec_labels})\n",
        "\n",
        "    with open(f\"{xml_path.split('.')[0]}.json\", 'w') as f:\n",
        "        json.dump(output, f, indent=4)"
      ],
      "metadata": {
        "id": "MuF_i_3qDE_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Stanford POS tagger"
      ],
      "metadata": {
        "id": "BAsDhI47E_IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip -P {local_path}/stanford_pos\n",
        "!unzip {local_path}/stanford_pos/stanford-postagger-full-2015-04-20.zip -d {local_path}/stanford_pos\n",
        "!ls {local_path}/stanford_pos/stanford-postagger-full-2015-04-20\n",
        "!export STANFORDTOOLSDIR=${local_path}/stanford_pos/stanford-postagger-full-2015-04-20\n",
        "!export CLASSPATH=${STANFORDTOOLSDIR}/stanford-postagger.jar\n",
        "!export STANFORD_MODELSDIR=${STANFORDTOOLSDIR}/models"
      ],
      "metadata": {
        "id": "XhDBQn71E7zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tag dataset"
      ],
      "metadata": {
        "id": "d1NdWqRPHEyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "\n",
        "def mask_and_label(sent):\n",
        "\tsent = ' ' + sent\n",
        "\tlower_sent = sent.lower()\n",
        "\n",
        "\tif ' than ' in lower_sent and all([exc not in lower_sent for exc in exclude]):\n",
        "\t\twords = tokenize.word_tokenize(sent)\n",
        "\t\tlowers = [word.lower() for word in words]\n",
        "\t\twords_ctr = Counter(lowers)\n",
        "\n",
        "\t\tif words_ctr['than'] == 1: # more than 1 are not useful (mostly describing only quantitative relations)\n",
        "\t\t\tthan_idx = lowers.index('than')\n",
        "\t\t\tinter = set(lowers[:than_idx]).intersection(key_words)\n",
        "\n",
        "\t\t\tif len(inter) >= 1:\n",
        "\t\t\t\tup_indices = [1 if word.lower() in ups else 0 for word in words]\n",
        "\t\t\t\tdown_indices = [1 if word.lower() in downs else 0 for word in words]\n",
        "\n",
        "\t\t\t\tif any(up_indices) and not any(down_indices):\n",
        "\t\t\t\t\tif than_idx + 1 < len(lowers) and (lowers[than_idx+1].isnumeric() or lowers[than_idx+1] in nums):\n",
        "\t\t\t\t\t\tpass\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tindices = [idx_ for idx_, up in enumerate(up_indices) if up == 1] + [than_idx]\n",
        "\t\t\t\t\t\tfinal = words\n",
        "\t\t\t\t\t\tdirection = 2\n",
        "\n",
        "\t\t\t\telif any(down_indices) and not any(up_indices):\n",
        "\t\t\t\t\tif than_idx + 1 < len(lowers) and (lowers[than_idx+1].isnumeric() or lowers[than_idx+1] in nums):\n",
        "\t\t\t\t\t\tpass\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tindices = [idx_ for idx_, down in enumerate(down_indices) if down == 1] + [than_idx]\n",
        "\t\t\t\t\t\tfinal = words\n",
        "\t\t\t\t\t\tdirection = 0\n",
        "\n",
        "\telif ' similar' in lower_sent and ' to ' in lower_sent:\n",
        "\t\twords = tokenize.word_tokenize(sent)\n",
        "\t\tlowers = [word.lower() for word in words]\n",
        "\t\twords_ctr = Counter(lowers)\n",
        "\n",
        "\t\tfor idx, lower in enumerate(lowers):\n",
        "\t\t\tif lower in sims:\n",
        "\t\t\t\tsim_idx = idx\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tif 'sim_idx' in locals():\n",
        "\t\t\tif 'to' in lowers[sim_idx:]:\n",
        "\t\t\t\tto_idx = sim_idx + lowers[sim_idx:].index('to')\n",
        "\n",
        "\t\t\t\tindices = [sim_idx] + [to_idx]\n",
        "\t\t\t\tfinal = words\n",
        "\t\t\t\tdirection = 1\n",
        "\n",
        "\telif ' no' in lower_sent and ' differ' in lower_sent and 'and' in lower_sent:\n",
        "\t\twords = tokenize.word_tokenize(sent)\n",
        "\t\tlowers = [word.lower() for word in words]\n",
        "\t\twords_ctr = Counter(lowers)\n",
        "\n",
        "\t\tfor idx, word in enumerate(lowers):\n",
        "\t\t\tif word in diffs:\n",
        "\t\t\t\tdiff_idx = idx\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tif 'diff_idx' in locals():\n",
        "\t\t\t# first find the left no, then scan the middle words\n",
        "\t\t\tfor i in range(idx):\n",
        "\t\t\t\tword = words[idx-1-i]\n",
        "\t\t\t\tif word in nos:\n",
        "\t\t\t\t\tno_idx = idx-1-i\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\tif 'no_idx' in locals():\n",
        "\t\t\t\tbet_indices = [1 if word == 'between' and idx > diff_idx else 0 for idx, word in enumerate(lowers)]\n",
        "\n",
        "\t\t\t\tif any(bet_indices):\n",
        "\t\t\t\t\tfirst_bet = bet_indices.index(1)\n",
        "\t\t\t\t\tif 'and' in lowers[first_bet:]:\n",
        "\t\t\t\t\t\tand_idx = first_bet + lowers[first_bet:].index('and')\n",
        "\t\t\t\t\t\tindices = list(range(no_idx, diff_idx+1)) + [idx for idx, bet in enumerate(bet_indices) if bet == 1] + [and_idx]\n",
        "\t\t\t\t\t\tfinal = words\n",
        "\t\t\t\t\t\tdirection = 1\n",
        "\n",
        "\tif 'final' in locals() and 'direction' in locals():\n",
        "\t\tif type(final) == list and len(final) > 0 and final[-1] != '?':\n",
        "\t\t\treturn [final, direction, indices]\n",
        "\n",
        "\telse:\n",
        "\t\treturn False"
      ],
      "metadata": {
        "id": "qxXtJ_q7IhAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "\n",
        "def process(item):\n",
        "\t# an item is an article\n",
        "\t# also need to save the context\n",
        "\t# as well as save the evidence\n",
        "\tpmid = item['pmid']\n",
        "\ttexts = item['texts']\n",
        "\tlabels = item['sec_labels']\n",
        "\n",
        "\tevi_output = []\n",
        "\tctx_output = {'pmid': pmid, 'ctx': ''}\n",
        "\n",
        "\tbg_status = True\n",
        "\n",
        "\tfor text, label in zip(texts, labels):\n",
        "\t\tif label == 'TITLE': continue\n",
        "\t\tsents = tokenize.sent_tokenize(text)\n",
        "\t\tif not label or label not in sec2label:\n",
        "\t\t\tfor sent in sents:\n",
        "\t\t\t\tresult = mask_and_label(sent)\n",
        "\t\t\t\tif result:\n",
        "\t\t\t\t\tbg_status = False\n",
        "\t\t\t\t\tevi_output.append({'pmid': pmid, 'pos': result[0], 'label': result[1], 'indices': result[2]})\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif bg_status:\n",
        "\t\t\t\t\t\tctx_output['ctx'] += ' ' + sent\n",
        "\t\telse:\n",
        "\t\t\tjudge = sec2label[label]\n",
        "\n",
        "\t\t\tif judge == '1': # all background\n",
        "\t\t\t\tctx_output['ctx'] += ' ' + text\n",
        "\t\t\telse:\n",
        "\t\t\t\tbg_status = False # starting no background\n",
        "\t\t\t\tfor sent in sents:\n",
        "\t\t\t\t\tresult = mask_and_label(sent)\n",
        "\t\t\t\t\tif result: evi_output.append({'pmid': pmid, 'pos': result[0], 'label': result[1], 'indices': result[2]})\n",
        "\n",
        "\treturn evi_output, ctx_output"
      ],
      "metadata": {
        "id": "ntF_vYqMImWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FIGURE OUT HOW TO DOWNLOAD sec2label.json and bad_pmids.json ##"
      ],
      "metadata": {
        "id": "sxp3bweJL0Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk.tag import StanfordPOSTagger\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sys\n",
        "\n",
        "import random as rd\n",
        "\n",
        "\n",
        "'''\n",
        "used to pseudo label the dataset\n",
        "'''\n",
        "\n",
        "st = StanfordPOSTagger('english-left3words-distsim.tagger', java_options='-mx4g')\n",
        "\n",
        "exclude = set(['rather than', 'other than'])\n",
        "ups = set(['better', 'greater', 'higher', 'later', 'more', 'faster', 'older', 'longer', \\\n",
        "\t\t'larger', 'broader', 'wider', 'stronger', 'deeper', 'more', 'commoner', 'richer', \\\n",
        "\t\t'further', 'bigger'])\n",
        "downs = set(['worse', 'smaller', 'lower', 'earlier', 'less', 'slower', 'younger', 'shorter', \\\n",
        "\t\t'smaller', 'narrower', 'narrower', 'weaker', 'shallower', 'fewer', 'rarer', 'poorer', \\\n",
        "\t\t'closer', 'smaller'])\n",
        "\n",
        "key_words = ups.union(downs)\n",
        "\n",
        "diffs = set(['difference', 'differences', 'different', 'differently', 'differ'])\n",
        "sims = set(['similar', 'similarly', 'similarity', 'similarities'])\n",
        "\n",
        "nos = set(['no', 'not'])\n",
        "middles = set(['significant', 'significantly', 'statistic', 'statistically', 'statistical'])\n",
        "\n",
        "nums = set([\"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\", \"zero\", \\\n",
        "            \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \\\n",
        "            \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\"])\n",
        "\n",
        "sec2label = json.load(open(f\"{local_path}/pretraining_dataset/sec2label.json\"))\n",
        "\n",
        "chunks = list(range(1, 1016))\n",
        "rd.shuffle(chunks)\n",
        "\n",
        "total = 0\n",
        "\n",
        "for idx, chunk_id in enumerate(chunks):\n",
        "\tif not os.path.exists(f\"{local_path}/pretraining_dataset/evidence/evidence_pos_{chunk_id:04d}.json\"):\n",
        "\t\tdata_path = f\"{local_path}/pretraining_dataset/evidence/pubmed24n{chunk_id:04d}.json\"\n",
        "\t\tif not os.path.exists(data_path): continue\n",
        "\n",
        "\t\tevi_output = []\n",
        "\t\tctx_output = []\n",
        "\t\tdata = json.load(open(data_path))\n",
        "\n",
        "\t\tfor item in data:\n",
        "\t\t\tresults = process(item)\n",
        "\t\t\tevi_output += results[0]\n",
        "\t\t\tctx_output.append(results[1])\n",
        "\n",
        "\t\tpos_list = st.tag_sents(o['pos'] for o in evi_output)\n",
        "\n",
        "\t\tfor _idx in range(len(evi_output)):\n",
        "\t\t\tevi_output[_idx]['pos'] = pos_list[_idx]\n",
        "\n",
        "\t\twith open(f\"{local_path}/pretraining_dataset/evidence/evidence_pos_{chunk_id:04d}.json\", 'w') as f:\n",
        "\t\t\tjson.dump(evi_output, f)\n",
        "\t\twith open(f\"{local_path}/pretraining_dataset/evidence/contexts_{chunk_id:04d}.json\", 'w') as f:\n",
        "\t\t\tjson.dump(ctx_output, f)\n",
        "\n",
        "\telse:\n",
        "\t\tevi_output = json.load(open(f\"{local_path}/pretraining_dataset/evidence/evidence_pos_{chunk_id:04d}.json\"))\n",
        "\n",
        "\ttotal += len(evi_output)\n",
        "\n",
        "\tprint('%d/%d; Processing %s; Number of evidence: %d; Total: %d' % (idx+1, len(chunks), chunk_id, len(evi_output), total))"
      ],
      "metadata": {
        "id": "639EfNT6HIN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process tags"
      ],
      "metadata": {
        "id": "ug7TTmr0O29-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "\n",
        "def reversed(words, label):\n",
        "\n",
        "\tall_rev = (words + ['MASK'])[::-1]\n",
        "\n",
        "\tmask_idx = [idx for idx, word in enumerate(all_rev) if word == '[MASK]']\n",
        "\tmask_idx = [0] + mask_idx + [len(all_rev)]\n",
        "\n",
        "\tfor i, idx in enumerate(mask_idx[:-1]):\n",
        "\t\tall_rev[idx+1: mask_idx[i+1]] = all_rev[idx+1: mask_idx[i+1]][::-1]\n",
        "\n",
        "\tall_rev = all_rev[1:]\n",
        "\n",
        "\tif label in up2down:\n",
        "\t\trev_label = up2down[label]\n",
        "\telif label in down2up:\n",
        "\t\trev_label = down2up[label]\n",
        "\telse:\n",
        "\t\trev_label = label\n",
        "\n",
        "\treturn all_rev, rev_label"
      ],
      "metadata": {
        "id": "5Doe_M-gPK7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "\n",
        "def get_label(pos, indices, label2ctr):\n",
        "\tind_words = [pos[ind][0] for ind in indices]\n",
        "\n",
        "\tif len(ind_words) == 2:\n",
        "\t\tlabel = ind_words[0].lower()\n",
        "\t\tif label not in label2idx:\n",
        "\t\t\treturn False\n",
        "\t\telse:\n",
        "\t\t\tlabel2ctr[label] += 1\n",
        "\t\t\treturn label2idx[label]\n",
        "\telse:\n",
        "\t\tif ind_words[-1] == 'than':\n",
        "\t\t\treturn False\n",
        "\t\telse:\n",
        "\t\t\tlabel2ctr['nodiff'] += 1\n",
        "\t\t\treturn label2idx['nodiff']"
      ],
      "metadata": {
        "id": "XKP-7ek4PL5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'\n",
        "\n",
        "# process labels\n",
        "ups = ['better', 'greater', 'higher', 'later', 'more', 'faster', 'older', 'longer', \\\n",
        "\t\t'larger', 'broader', 'wider', 'stronger', 'deeper', 'more', 'commoner', 'richer', \\\n",
        "\t\t'further', 'bigger']\n",
        "downs = ['worse', 'smaller', 'lower', 'earlier', 'less', 'slower', 'younger', 'shorter', \\\n",
        "\t\t'smaller', 'narrower', 'narrower', 'weaker', 'shallower', 'fewer', 'rarer', 'poorer', \\\n",
        "\t\t'closer', 'smaller']\n",
        "sims = ['nodiff', 'similar']\n",
        "\n",
        "label_set =  list(set(downs)) + list(set(sims)) + list(set(ups))\n",
        "\n",
        "label2idx = {label: idx for idx, label in enumerate(label_set)}\n",
        "label2idx['similarly'] = label2idx['similar']\n",
        "label2idx['similarity'] = label2idx['similar']\n",
        "label2idx['similarities'] = label2idx['similar']\n",
        "label2idx['farther'] = label2idx['further']\n",
        "\n",
        "label2ctr = {k: 0 for k in list(label2idx)}\n",
        "\n",
        "up2down = {label2idx[k]: label2idx[v] for k, v in zip(ups, downs)}\n",
        "down2up = {label2idx[k]: label2idx[v] for k, v in zip(downs, ups)}\n",
        "\n",
        "# start\n",
        "output = []\n",
        "\n",
        "removed = set(['CD'])\n",
        "indicators = set(['significant', 'significantly', 'statistically', 'statistic', '%'])\n",
        "sims = set(['similar', 'similarly', 'similarity', 'similarities'])\n",
        "\n",
        "pmids = set()\n",
        "\n",
        "for chunk_id in range(1, 1016):\n",
        "\tdata_path = f\"{local_path}/pretraining_dataset/evidence/evidence_pos_{chunk_id:04d}.json\"\n",
        "\tif not os.path.exists(data_path): continue\n",
        "\n",
        "\tdata = json.load(open(data_path))\n",
        "\n",
        "\tfor item in data:\n",
        "\t\t# first detect parentheses\n",
        "\t\tpmid = item['pmid']\n",
        "\t\tpos = item['pos']\n",
        "\t\tindices = item['indices']\n",
        "\n",
        "\t\tlabel = get_label(pos, indices, label2ctr)\n",
        "\n",
        "\t\tif not label: continue # lose about ~20%\n",
        "\n",
        "\t\tpar_stack = []\n",
        "\t\tidx_stack = []\n",
        "\t\tlefts = []\n",
        "\t\trights = []\n",
        "\t\tfor idx, info in enumerate(pos):\n",
        "\t\t\tif info[0] in {'(', ')'}:\n",
        "\t\t\t\tif not par_stack:\n",
        "\t\t\t\t\tif info[0] == ')': continue\n",
        "\t\t\t\t\tpar_stack.append(info[0])\n",
        "\t\t\t\t\tidx_stack.append(idx)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif par_stack[-1] == info[0]:\n",
        "\t\t\t\t\t\tpar_stack.append(info[0])\n",
        "\t\t\t\t\t\tidx_stack.append(idx)\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tpar_stack = par_stack[:-1]\n",
        "\t\t\t\t\t\tlefts.append(idx_stack[-1])\n",
        "\t\t\t\t\t\trights.append(idx)\n",
        "\t\t\t\t\t\tidx_stack = idx_stack[:-1]\n",
        "\n",
        "\t\twithin_par = []\n",
        "\t\tif lefts and rights:\n",
        "\t\t\tfor left, right in zip(lefts, rights):\n",
        "\t\t\t\twithin_par += list(range(left, right+1))\n",
        "\n",
        "\t\t# detect irrelavent subsentences\n",
        "\t\tdot_indices = [idx for idx, info in enumerate(pos) if info[0] == ',']\n",
        "\t\touter_idx = []\n",
        "\t\tif dot_indices:\n",
        "\t\t\tleft, right = min(item['indices']), max(item['indices'])\n",
        "\t\t\t# item['indices'] save the important indices\n",
        "\t\t\tdot_indices = [-1] + dot_indices + [len(pos)]\n",
        "\t\t\t# print(left, right, dot_indices)\n",
        "\t\t\tfor i in range(len(dot_indices)-1):\n",
        "\t\t\t\tif dot_indices[i] <= left < dot_indices[i+1]:\n",
        "\t\t\t\t\tleft_start = i\n",
        "\t\t\t\tif dot_indices[i] <= right < dot_indices[i+1]:\n",
        "\t\t\t\t\tright_start = i\n",
        "\t\t\tleft = dot_indices[left_start]\n",
        "\t\t\tright = dot_indices[right_start+1]\n",
        "\t\t\tfor i in range(len(pos)):\n",
        "\t\t\t\tif i <= left or i >= right:\n",
        "\t\t\t\t\touter_idx.append(i)\n",
        "\n",
        "\t\t# detect irrelavent show that / suggest that\n",
        "\n",
        "\t\t# RB before JJR in generally bad\n",
        "\t\tinclude_idx = []\n",
        "\t\tthat_judged = False # only judge once\n",
        "\t\tfor idx, i in enumerate(pos):\n",
        "\t\t\tif idx in outer_idx:\n",
        "\t\t\t\t#print(i, '----------OUT')\n",
        "\t\t\t\tpass\n",
        "\t\t\telif idx+1 < len(pos) and (pos[idx+1][1] == 'JJR' or pos[idx+1][1] == 'RBR') and \\\n",
        "\t\t\t\t((i[1] == 'RB' and i[0].lower() != 'not') \\\n",
        "\t\t\t\tor i[0].lower() == 'times'):\n",
        "\t\t\t\t#print(i, '----------FRONT_RB')\n",
        "\t\t\t\tpass\n",
        "\t\t\telif idx in item['indices']:\n",
        "\t\t\t\t#print(i, '----------DETECTED')\n",
        "\t\t\t\tpass\n",
        "\t\t\telif i[1] in removed:\n",
        "\t\t\t\t#print(i, '----------TOREMOVE')\n",
        "\t\t\t\tpass\n",
        "\t\t\telif i[0].lower() in indicators:\n",
        "\t\t\t\t#print(i, '----------INDICATOR')\n",
        "\t\t\t\tpass\n",
        "\t\t\telif idx in within_par:\n",
        "\t\t\t\t#print(i, '----------INPAR')\n",
        "\t\t\t\tpass\n",
        "\t\t\telif not that_judged and i[0].lower() == 'that':\n",
        "\t\t\t\tif idx < min(item['indices']):\n",
        "\t\t\t\t\t#print(i, '----------THAT')\n",
        "\t\t\t\t\tthat_judged = True\n",
        "\t\t\t\t\tinclude_idx = []\n",
        "\t\t\telse:\n",
        "\t\t\t\t#print(i)\n",
        "\t\t\t\tinclude_idx.append(idx)\n",
        "\n",
        "\t\tfinal_evidence = []\n",
        "\t\tfor idx, i in enumerate(pos):\n",
        "\n",
        "\t\t\tif idx in include_idx:\n",
        "\t\t\t\tfinal_evidence.append(i[0])\n",
        "\t\t\telse:\n",
        "\t\t\t\tif  final_evidence and final_evidence[-1] != '[MASK]':\n",
        "\t\t\t\t\tfinal_evidence.append('[MASK]')\n",
        "\n",
        "\t\tif not final_evidence: continue\n",
        "\n",
        "\t\tif final_evidence[-1] in ['.', '[MASK]']:\n",
        "\t\t\tfinal_evidence = final_evidence[:-1]\n",
        "\n",
        "\t\t# Make every word after [MASK] upper cased\n",
        "\t\tfor idx, word in enumerate(final_evidence):\n",
        "\t\t\tif idx == 0 and word != '[MASK]':\n",
        "\t\t\t\tfinal_evidence[idx] = word[0].upper() + word[1:]\n",
        "\t\t\telif word == '[MASK]' and idx + 1 < len(final_evidence) and final_evidence[idx+1]:\n",
        "\t\t\t\tfinal_evidence[idx+1] = final_evidence[idx+1][0].upper() + final_evidence[idx+1][1:]\n",
        "\n",
        "\t\trev_evidence, rev_label = reversed(final_evidence, label)\n",
        "\n",
        "\t\toutput.append({'pmid': pmid,\n",
        "\t\t\t\t'pico': ' '.join(final_evidence), 'label': label,\n",
        "\t\t\t\t'rev_pico': ' '.join(rev_evidence), 'rev_label': rev_label})\n",
        "\n",
        "\t\tpmids.add(pmid)\n",
        "\n",
        "\tprint('Processed chunk #%d. Got %d insts' % (chunk_id, len(output)))\n",
        "\n",
        "with open(f'{local_path}/evidence.json', 'w') as f:\n",
        "\tjson.dump(output, f, indent=4)\n",
        "\n",
        "with open(f'{local_path}/evidence_pmids.json', 'w') as f:\n",
        "\tjson.dump(list(pmids), f, indent=4)"
      ],
      "metadata": {
        "id": "C2LB631GO4mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate contexts"
      ],
      "metadata": {
        "id": "X2cHx_1jQj72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'\n",
        "\n",
        "'''\n",
        "codes to aggregate the contexts\n",
        "only aggregate the needed contexts\n",
        "'''\n",
        "\n",
        "ctxs = glob.glob(f'{local_path}/pretraining_dataset/evidence/contexts_*')\n",
        "\n",
        "pmids = set(json.load(open(f'{local_path}/pretraining_dataset/evidence_pmids.json')))\n",
        "bad_pmids = set(json.load(open(f'{local_path}/pretraining_dataset/bad_pmids.json')))\n",
        "\n",
        "pmid2ctx = {}\n",
        "\n",
        "for ctx in ctxs:\n",
        "\tprint('Processing %s' % ctx)\n",
        "\tdata = json.load(open(ctx))\n",
        "\n",
        "\tfor item in data:\n",
        "\t\tpmid = item['pmid']\n",
        "\t\tctx = item['ctx']\n",
        "\n",
        "\t\tif pmid not in pmids or pmid in bad_pmids: continue\n",
        "\n",
        "\t\tpmid2ctx[pmid] = ctx\n",
        "\n",
        "with open(f'{local_path}/pretraining_dataset/pmid2ctx.json', 'w') as f:\n",
        "\tjson.dump(pmid2ctx, f, indent=4)"
      ],
      "metadata": {
        "id": "CFm9XOtwQjgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index dataset"
      ],
      "metadata": {
        "id": "OMjPsnSQRGEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'\n",
        "\n",
        "pmid2ctxid = {}\n",
        "\n",
        "pmid2ctx = json.load(open(f'{local_path}/pretraining_dataset/pmid2ctx.json'))\n",
        "evidence = json.load(open(f'{local_path}/pretraining_dataset/evidence.json'))\n",
        "\n",
        "indexed_evidence = []\n",
        "indexed_contexts = []\n",
        "\n",
        "for entry in evidence:\n",
        "\tpmid = entry['pmid']\n",
        "\tif pmid not in pmid2ctx: continue\n",
        "\n",
        "\tif pmid not in pmid2ctxid:\n",
        "\t\tpmid2ctxid[pmid] = len(pmid2ctxid)\n",
        "\t\tindexed_contexts.append({'passage': pmid2ctx[pmid], 'ctx_id': pmid2ctxid[pmid]})\n",
        "\n",
        "\tentry['ctx_id'] = pmid2ctxid[pmid]\n",
        "\n",
        "\tindexed_evidence.append(entry)\n",
        "\n",
        "with open(f'{local_path}/pretraining_dataset/indexed_evidence.json', 'w') as f:\n",
        "\tjson.dump(indexed_evidence, f, indent=4)\n",
        "with open(f'{local_path}/pretraining_dataset/indexed_contexts.json', 'w') as f:\n",
        "\tjson.dump(indexed_contexts, f, indent=4)"
      ],
      "metadata": {
        "id": "6b0bbYvrREKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run EBM-Net"
      ],
      "metadata": {
        "id": "gJzrgHkgSAqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ],
      "metadata": {
        "id": "kIqIoIoyTF8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## NEED TO ADD IMPORTS ##"
      ],
      "metadata": {
        "id": "kvKTIMlkVLod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(args):\n",
        "\trd.seed(args.seed)\n",
        "\tnp.random.seed(args.seed)\n",
        "\ttorch.manual_seed(args.seed)\n",
        "\tif args.n_gpu > 0:\n",
        "\t\ttorch.cuda.manual_seed_all(args.seed)"
      ],
      "metadata": {
        "id": "6nS6oefzSYsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_list(tensor):\n",
        "\treturn tensor.detach().cpu().tolist()"
      ],
      "metadata": {
        "id": "sjFA4sYkSdnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "def train(args, train_picos, train_ctxs, model, tokenizer):\n",
        "\t\"\"\" Train the model \"\"\"\n",
        "\t#tb_writer = SummaryWriter()\n",
        "\n",
        "\targs.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\ttrain_sampler = RandomSampler(train_picos)\n",
        "\ttrain_dataloader = DataLoader(train_picos, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "\tif args.max_steps > 0:\n",
        "\t\tt_total = args.max_steps\n",
        "\t\targs.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "\telse:\n",
        "\t\tt_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "\t# Prepare optimizer and schedule (linear warmup and decay)\n",
        "\tno_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\toptimizer_grouped_parameters = [\n",
        "\t\t{\n",
        "\t\t\t\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "\t\t\t\"weight_decay\": args.weight_decay,\n",
        "\t\t},\n",
        "\t\t{\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "\t]\n",
        "\toptimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "\tscheduler = get_cosine_schedule_with_warmup(\n",
        "\t\toptimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "\t)\n",
        "\n",
        "\t# multi-gpu training\n",
        "\tif args.n_gpu > 1:\n",
        "\t\tmodel = torch.nn.DataParallel(model)\n",
        "\n",
        "\t# Train!\n",
        "\tlogger.info(\"***** Running training *****\")\n",
        "\tlogger.info(\"  Num examples = %d\", len(train_picos))\n",
        "\tlogger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "\tlogger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "\tlogger.info(\n",
        "\t\t\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "\t\targs.train_batch_size\n",
        "\t\t* args.gradient_accumulation_steps)\n",
        "\tlogger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "\tlogger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "\tglobal_step = 0\n",
        "\ttr_loss, logging_loss = 0.0, 0.0\n",
        "\tmodel.zero_grad()\n",
        "\ttrain_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=False)\n",
        "\tset_seed(args)\t# Added here for reproductibility\n",
        "\tfor _ in train_iterator:\n",
        "\t\tepoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
        "\t\tfor step, batch in enumerate(epoch_iterator):\n",
        "\t\t\tmodel.train()\n",
        "\n",
        "\t\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "\t\t\tctx_ids = to_list(batch[0])\n",
        "\t\t\tpico_token_ids = batch[1] # B x max_pico_length\n",
        "\t\t\tpico_token_mask = batch[2] # B x max_pico_length\n",
        "\t\t\tpico_segment_ids = batch[3] # B x max_pico_length\n",
        "\t\t\tlabels = batch[4]\n",
        "\n",
        "\t\t\tctx_batch = [train_ctxs[ctx_id] for ctx_id in ctx_ids] # B x list of ctx dataset\n",
        "\t\t\tctx_batch = list(map(list, zip(*ctx_batch)))\n",
        "\n",
        "\t\t\tctx_token_ids = torch.stack(ctx_batch[1]).to(args.device) # B x max_ctx_length\n",
        "\t\t\tctx_token_mask = torch.stack(ctx_batch[2]).to(args.device) # B x max_ctx_length\n",
        "\t\t\tctx_segment_ids = torch.stack(ctx_batch[3]).to(args.device) # B x max_ctx_length\n",
        "\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t\"passage_ids\": torch.cat([ctx_token_ids, pico_token_ids], dim=1),\n",
        "\t\t\t\t\"passage_mask\": torch.cat([ctx_token_mask, pico_token_mask], dim=1),\n",
        "\t\t\t\t\"passage_segment_ids\": torch.cat([ctx_segment_ids, pico_segment_ids], dim=1),\n",
        "\t\t\t\t\"result_labels\": labels\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\toutputs = model(inputs)\n",
        "\n",
        "\t\t\tloss = outputs  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "\t\t\tif args.n_gpu > 1:\n",
        "\t\t\t\tloss = loss.mean()\t# mean() to average on multi-gpu parallel (not distributed) training\n",
        "\t\t\tif args.gradient_accumulation_steps > 1:\n",
        "\t\t\t\tloss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "\t\t\tloss.backward()\n",
        "\n",
        "\t\t\ttr_loss += loss.item()\n",
        "\n",
        "\t\t\tif (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "\t\t\t\toptimizer.step()\n",
        "\t\t\t\tscheduler.step()  # Update learning rate schedule\n",
        "\t\t\t\tmodel.zero_grad()\n",
        "\t\t\t\tglobal_step += 1\n",
        "\n",
        "\t\t\t\tif args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "\t\t\t\t\t# Log metrics\n",
        "\t\t\t\t\t#tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "\t\t\t\t\t#tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "\t\t\t\t\t#print((tr_loss - logging_loss) / args.logging_steps)\n",
        "\t\t\t\t\tlogging_loss = tr_loss\n",
        "\n",
        "\t\t\t\tif args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\t\t\t\t\t# Save model checkpoint\n",
        "\t\t\t\t\toutput_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "\t\t\t\t\tif not os.path.exists(output_dir):\n",
        "\t\t\t\t\t\tos.makedirs(output_dir)\n",
        "\t\t\t\t\tmodel_to_save = (\n",
        "\t\t\t\t\t\tmodel.module if hasattr(model, \"module\") else model\n",
        "\t\t\t\t\t)  # Take care of distributed/parallel training\n",
        "\t\t\t\t\tmodel_to_save.save_pretrained(output_dir)\n",
        "\t\t\t\t\ttokenizer.save_pretrained(output_dir)\n",
        "\t\t\t\t\ttorch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "\t\t\t\t\tlogger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "\t\t\tif args.max_steps > 0 and global_step > args.max_steps:\n",
        "\t\t\t\tepoch_iterator.close()\n",
        "\t\t\t\tbreak\n",
        "\t\tif args.max_steps > 0 and global_step > args.max_steps:\n",
        "\t\t\ttrain_iterator.close()\n",
        "\t\t\tbreak\n",
        "\n",
        "\t#tb_writer.close()\n",
        "\n",
        "\treturn global_step, tr_loss / global_step"
      ],
      "metadata": {
        "id": "Pf_C-X8tSiMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(args, eval_picos, eval_ctxs, model, tokenizer, prefix=\"\"):\n",
        "\n",
        "\tif not os.path.exists(args.output_dir):\n",
        "\t\tos.makedirs(args.output_dir)\n",
        "\n",
        "\targs.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "\t# Note that DistributedSampler samples randomly\n",
        "\teval_sampler = SequentialSampler(eval_picos)\n",
        "\teval_dataloader = DataLoader(eval_picos, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "\t# Eval!\n",
        "\tlogger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "\tlogger.info(\"  Num examples = %d\", len(eval_picos))\n",
        "\tlogger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "\texample_ids = []\n",
        "\tall_labels = []\n",
        "\tall_preds = []\n",
        "\tall_logits = np.zeros((0, 3))\n",
        "\n",
        "\tfor batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "\t\tmodel.eval()\n",
        "\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tctx_ids = to_list(batch[0])\n",
        "\t\t\tpico_token_ids = batch[1] # B x max_pico_length\n",
        "\t\t\tpico_token_mask = batch[2] # B x max_pico_length\n",
        "\t\t\tpico_segment_ids = batch[3] # B x max_pico_length\n",
        "\t\t\tlabels = batch[4]\n",
        "\n",
        "\t\t\tctx_batch = [eval_ctxs[ctx_id] for ctx_id in ctx_ids] # B x list of ctx dataset\n",
        "\t\t\tctx_batch = list(map(list, zip(*ctx_batch)))\n",
        "\n",
        "\t\t\tctx_token_ids = torch.stack(ctx_batch[1]).to(args.device) # B x max_ctx_length\n",
        "\t\t\tctx_token_mask = torch.stack(ctx_batch[2]).to(args.device) # B x max_ctx_length\n",
        "\t\t\tctx_segment_ids = torch.stack(ctx_batch[3]).to(args.device) # B x max_ctx_length\n",
        "\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t\"passage_ids\": torch.cat([ctx_token_ids, pico_token_ids], dim=1),\n",
        "\t\t\t\t\"passage_mask\": torch.cat([ctx_token_mask, pico_token_mask], dim=1),\n",
        "\t\t\t\t\"passage_segment_ids\": torch.cat([ctx_segment_ids, pico_segment_ids], dim=1)\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\tlogits = model(inputs) # N x 3\n",
        "\t\t\tpreds = torch.argmax(logits, dim=1) # N\n",
        "\n",
        "\t\t\texample_ids += list(batch[4].detach().cpu().numpy())\n",
        "\t\t\tall_labels += list(labels.detach().cpu().numpy())\n",
        "\t\t\tall_preds += list(preds.detach().cpu().numpy())\n",
        "\t\t\tall_logits = np.concatenate([all_logits, logits.detach().cpu().numpy()], axis=0)\n",
        "\n",
        "\tif not prefix:\n",
        "\t\tprefix = 'final'\n",
        "\n",
        "\twith open(os.path.join(args.output_dir, '%s_all_example_idx.json' % prefix), 'w') as f:\n",
        "\t\tjson.dump([int(label) for label in example_ids], f)\n",
        "\twith open(os.path.join(args.output_dir, '%s_all_labels.json' % prefix), 'w') as f:\n",
        "\t\tjson.dump([int(label) for label in all_labels], f)\n",
        "\twith open(os.path.join(args.output_dir, '%s_all_preds.json' % prefix), 'w') as f:\n",
        "\t\tjson.dump([int(pred) for pred in all_preds], f)\n",
        "\tnp.save(os.path.join(args.output_dir, '%s_all_logits.npy' % prefix), np.array(all_logits))\n",
        "\n",
        "\tresults = {}\n",
        "\tresults['f1'] = f1_score(all_labels, all_preds, average='macro')\n",
        "\tresults['acc'] = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "\treturn results"
      ],
      "metadata": {
        "id": "hb1SMBMbSmgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def represent(args, model, tokenizer):\n",
        "\tdataset = load_and_cache_examples(args, tokenizer, evaluate=True, do_repr=True)\n",
        "\n",
        "\tif not os.path.exists(args.output_dir):\n",
        "\t\tos.makedirs(args.output_dir)\n",
        "\n",
        "\targs.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "\t# Note that DistributedSampler samples randomly\n",
        "\teval_sampler = SequentialSampler(dataset)\n",
        "\teval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "\t# Eval!\n",
        "\tlogger.info(\"***** Running Representations *****\")\n",
        "\tlogger.info(\"  Num examples = %d\", len(dataset))\n",
        "\tlogger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "\texample_ids = []\n",
        "\tall_reprs = np.zeros((0, model.bert.config.hidden_size))\n",
        "\n",
        "\tfor batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "\t\tmodel.eval()\n",
        "\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t\"passage_ids\": batch[0],\n",
        "\t\t\t\t\"passage_mask\": batch[1],\n",
        "\t\t\t\t\"passage_segment_ids\": batch[2],\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\treprs = model(inputs, get_reprs=True) # N x D\n",
        "\n",
        "\t\t\texample_ids += list(batch[4].detach().cpu().numpy())\n",
        "\t\t\tall_reprs = np.concatenate([all_reprs, reprs.detach().cpu().numpy()], axis=0)\n",
        "\n",
        "\twith open(os.path.join(args.output_dir, 'all_example_idx.json'), 'w') as f:\n",
        "\t\tjson.dump([int(_id) for _id in example_ids], f)\n",
        "\tnp.save(os.path.join(args.output_dir, 'all_reprs.npy'), np.array(all_reprs))"
      ],
      "metadata": {
        "id": "PXC89cO8SqVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_cache_ctxs(args, tokenizer, evaluate=False, do_repr=False, pretraining=False):\n",
        "\tif args.pretraining:\n",
        "\t\tfrom utils_pretraining import (\n",
        "\t\t\tconvert_ctxs_to_features,\n",
        "\t\t\tconvert_picos_to_features,\n",
        "\t\t\tread_ctx_examples,\n",
        "\t\t\tread_pico_examples)\n",
        "\telse:\n",
        "\t\tfrom utils_ebmnet import (\n",
        "\t\t\tconvert_ctxs_to_features,\n",
        "\t\t\tconvert_picos_to_features,\n",
        "\t\t\tread_ctx_examples,\n",
        "\t\t\tread_pico_examples)\n",
        "\n",
        "\t# We need to index it\n",
        "\n",
        "\t# Load data features from cache or dataset file\n",
        "\tif do_repr:\n",
        "\t\tinput_file = args.repr_ctx\n",
        "\telse:\n",
        "\t\tinput_file = args.predict_ctx if evaluate else args.train_ctx\n",
        "\n",
        "\tcached_features_file = os.path.join(\n",
        "\t\tos.path.dirname(input_file),\n",
        "\t\t\"cached_ctxs_adv{}_{}_{}\".format(\n",
        "\t\t\targs.adversarial,\n",
        "\t\t\t\"dev\" if evaluate else \"train\",\n",
        "\t\t\tstr(args.max_passage_length)\n",
        "\t\t),\n",
        "\t)\n",
        "\n",
        "\tif os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "\t\tlogger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "\t\tfeatures = torch.load(cached_features_file)\n",
        "\telse:\n",
        "\t\tlogger.info(\"Creating features from dataset file at %s\", input_file)\n",
        "\n",
        "\t\texamples = read_ctx_examples(input_file=input_file, adversarial=args.adversarial)\n",
        "\n",
        "\t\tfeatures = convert_ctxs_to_features(\n",
        "\t\t\texamples=examples,\n",
        "\t\t\ttokenizer=tokenizer,\n",
        "\t\t\tmax_passage_length=args.max_passage_length\n",
        "\t\t)\n",
        "\n",
        "\t\tlogger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "\t\ttorch.save(features, cached_features_file)\n",
        "\n",
        "\t# Convert to Tensors and build dataset\n",
        "\tall_ctx_ids = torch.tensor([f.ctx_id for f in features], dtype=torch.long)\n",
        "\tall_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "\tall_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "\tall_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "\n",
        "\tdataset = TensorDataset(\n",
        "\t\tall_ctx_ids,\n",
        "\t\tall_input_ids,\n",
        "\t\tall_input_mask,\n",
        "\t\tall_segment_ids\n",
        "\t)\n",
        "\n",
        "\treturn dataset"
      ],
      "metadata": {
        "id": "51HeU_9FSrRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_cache_picos(args, tokenizer, evaluate=False, do_repr=False, pretraining=False):\n",
        "\tif args.pretraining:\n",
        "\t\tfrom utils_pretraining import (\n",
        "\t\t\tconvert_ctxs_to_features,\n",
        "\t\t\tconvert_picos_to_features,\n",
        "\t\t\tread_ctx_examples,\n",
        "\t\t\tread_pico_examples)\n",
        "\telse:\n",
        "\t\tfrom utils_ebmnet import (\n",
        "\t\t\tconvert_ctxs_to_features,\n",
        "\t\t\tconvert_picos_to_features,\n",
        "\t\t\tread_ctx_examples,\n",
        "\t\t\tread_pico_examples)\n",
        "\t# Dataset that we are going to use\n",
        "\n",
        "\t# Load data features from cache or dataset file\n",
        "\tif do_repr:\n",
        "\t\tinput_file = args.repr_pico\n",
        "\telse:\n",
        "\t\tinput_file = args.predict_pico if evaluate else args.train_pico\n",
        "\n",
        "\tcached_features_file = os.path.join(\n",
        "\t\tos.path.dirname(input_file),\n",
        "\t\t\"cached_picos_adv{}_{}_{}_{}\".format(\n",
        "\t\t\targs.adversarial,\n",
        "\t\t\targs.permutation,\n",
        "\t\t\t\"dev\" if evaluate else \"train\",\n",
        "\t\t\tstr(args.max_pico_length)\n",
        "\t\t),\n",
        "\t)\n",
        "\n",
        "\tif os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "\t\tlogger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "\t\tfeatures = torch.load(cached_features_file)\n",
        "\telse:\n",
        "\t\tlogger.info(\"Creating features from dataset file at %s\", input_file)\n",
        "\n",
        "\t\texamples = read_pico_examples(input_file=input_file, adversarial=args.adversarial)\n",
        "\n",
        "\t\tfeatures = convert_picos_to_features(\n",
        "\t\t\texamples=examples,\n",
        "\t\t\ttokenizer=tokenizer,\n",
        "\t\t\tmax_pico_length=args.max_pico_length,\n",
        "\t\t\tpermutation=args.permutation\n",
        "\t\t)\n",
        "\n",
        "\t\tlogger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "\t\ttorch.save(features, cached_features_file)\n",
        "\n",
        "\n",
        "\t# Convert to Tensors and build dataset\n",
        "\tall_ctx_ids = torch.tensor([f.ctx_id for f in features], dtype=torch.long)\n",
        "\tall_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "\tall_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "\tall_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "\n",
        "\tmlm2cls = {}\n",
        "\tfor i in range(34):\n",
        "\t\tif i < 15:\n",
        "\t\t\tmlm2cls[i] = 0\n",
        "\t\telif 15 <= i < 17:\n",
        "\t\t\tmlm2cls[i] = 1\n",
        "\t\telse:\n",
        "\t\t\tmlm2cls[i] = 2\n",
        "\n",
        "\tif args.num_labels == 3 and args.pretraining: # here we have 34 labels to be processed\n",
        "\t\tall_labels = torch.tensor([mlm2cls[f.label] for f in features], dtype=torch.long)\n",
        "\telse:\n",
        "\t\tall_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "\n",
        "\tall_example_ids = torch.tensor([f.example_index for f in features], dtype=torch.long)\n",
        "\n",
        "\tdataset = TensorDataset(\n",
        "\t\tall_ctx_ids,\n",
        "\t\tall_input_ids,\n",
        "\t\tall_input_mask,\n",
        "\t\tall_segment_ids,\n",
        "\t\tall_labels,\n",
        "\t\tall_example_ids\n",
        "\t)\n",
        "\n",
        "\treturn dataset"
      ],
      "metadata": {
        "id": "NeMlspj7SzDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "\tAdamW,\n",
        "\tBertConfig,\n",
        "\tBertForSequenceClassification,\n",
        "\tBertTokenizer,\n",
        "\tget_cosine_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "import models\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def main():\n",
        "\tparser = argparse.ArgumentParser()\n",
        "\n",
        "\t# Required parameters\n",
        "\tparser.add_argument(\n",
        "\t\t\"--model_name_or_path\",\n",
        "\t\tdefault=None,\n",
        "\t\ttype=str,\n",
        "\t\trequired=True,\n",
        "\t\thelp='The path of the pre-trained model.'\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--output_dir\",\n",
        "\t\tdefault=None,\n",
        "\t\ttype=str,\n",
        "\t\trequired=True,\n",
        "\t\thelp=\"The output directory where the model checkpoints and predictions will be written.\",\n",
        "\t)\n",
        "\n",
        "\t# Other parameters\n",
        "\tparser.add_argument(\n",
        "\t\t\"--train_ctx\", default=None, type=str, help=\"json file for training\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--predict_ctx\", default=None, type=str, help=\"json for predictions\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--repr_ctx\", default=None, type=str, help=\"json for representatins\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--train_pico\", default=None, type=str, help=\"json for training\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--predict_pico\", default=None, type=str, help=\"json for predictions\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--repr_pico\", default=None, type=str, help=\"json for representatins\"\n",
        "\t)\n",
        "\n",
        "\tparser.add_argument(\n",
        "\t\t\"--permutation\",\n",
        "\t\tdefault=\"ioc\",\n",
        "\t\ttype=str,\n",
        "\t\thelp=\"The sequence of intervention, comparison and outcome\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--tokenizer_name\",\n",
        "\t\tdefault=\"\",\n",
        "\t\ttype=str,\n",
        "\t\thelp=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--cache_dir\",\n",
        "\t\tdefault=\"\",\n",
        "\t\ttype=str,\n",
        "\t\thelp=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--max_passage_length\",\n",
        "\t\tdefault=256,\n",
        "\t\ttype=int,\n",
        "\t\thelp=\"max length of passage.\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--max_pico_length\",\n",
        "\t\tdefault=128,\n",
        "\t\ttype=int,\n",
        "\t\thelp=\"max length of pico.\"\n",
        "\t)\n",
        "\tparser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "\tparser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "\tparser.add_argument(\"--do_repr\", action=\"store_true\", help=\"Whether to get representations\")\n",
        "\tparser.add_argument(\n",
        "\t\t\"--evaluate_during_training\", action=\"store_true\", help=\"Rul evaluation during training at each logging step.\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
        "\t)\n",
        "\tparser.add_argument(\"--per_gpu_train_batch_size\", default=24, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
        "\tparser.add_argument(\n",
        "\t\t\"--per_gpu_eval_batch_size\", default=24, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
        "\t)\n",
        "\tparser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "\tparser.add_argument(\n",
        "\t\t\"--gradient_accumulation_steps\",\n",
        "\t\ttype=int,\n",
        "\t\tdefault=1,\n",
        "\t\thelp=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "\t)\n",
        "\tparser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight deay if we apply some.\")\n",
        "\tparser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "\tparser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "\tparser.add_argument(\n",
        "\t\t\"--num_train_epochs\", default=24.0, type=float, help=\"Total number of training epochs to perform.\"\n",
        "\t)\n",
        "\tparser.add_argument(\n",
        "\t\t\"--max_steps\",\n",
        "\t\tdefault=-1,\n",
        "\t\ttype=int,\n",
        "\t\thelp=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "\t)\n",
        "\tparser.add_argument(\"--warmup_steps\", default=400, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "\tparser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n",
        "\tparser.add_argument(\"--save_steps\", type=int, default=25, help=\"Save checkpoint every X updates steps.\")\n",
        "\tparser.add_argument(\n",
        "\t\t\"--eval_all_checkpoints\",\n",
        "\t\taction=\"store_true\",\n",
        "\t\thelp=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
        "\t)\n",
        "\tparser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n",
        "\tparser.add_argument(\n",
        "\t\t\"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
        "\t)\n",
        "\tparser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\tparser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n",
        "\tparser.add_argument(\"--pretraining\", action=\"store_true\", help='Whether to do pre-training')\n",
        "\tparser.add_argument(\"--num_labels\", type=int, default=3, help='Number of labels at the last layer. Use 34 in pre-training and 3 in fine-tuning.')\n",
        "\tparser.add_argument(\"--adversarial\", action=\"store_true\", help='Whether using the adversarial setting.')\n",
        "\targs = parser.parse_args()\n",
        "\n",
        "\targs.overwrite_output_dir = True # always\n",
        "\tif (\n",
        "\t\tos.path.exists(args.output_dir)\n",
        "\t\tand os.listdir(args.output_dir)\n",
        "\t\tand args.do_train\n",
        "\t\tand not args.overwrite_output_dir\n",
        "\t):\n",
        "\t\traise ValueError(\n",
        "\t\t\t\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "\t\t\t\targs.output_dir\n",
        "\t\t\t)\n",
        "\t\t)\n",
        "\n",
        "\t# Setup CUDA, GPU & distributed training\n",
        "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "\targs.n_gpu = torch.cuda.device_count()\n",
        "\n",
        "\targs.device = device\n",
        "\n",
        "\t# Setup logging\n",
        "\tlogging.basicConfig(\n",
        "\t\tformat=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "\t\tdatefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "\t\tlevel=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "\t)\n",
        "\tlogger.warning(\n",
        "\t\t\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n",
        "\t\targs.local_rank,\n",
        "\t\tdevice,\n",
        "\t\targs.n_gpu,\n",
        "\t\tbool(args.local_rank != -1)\n",
        "\t)\n",
        "\n",
        "\t# Set seed\n",
        "\tset_seed(args)\n",
        "\n",
        "\ttokenizer = BertTokenizer.from_pretrained(\n",
        "\t\targs.model_name_or_path,\n",
        "\t\tdo_lower_case=args.do_lower_case\n",
        "\t)\n",
        "\n",
        "\tmodel = models.EBM_Net(args, path=args.model_name_or_path)\n",
        "\tmodel.to(args.device)\n",
        "\n",
        "\tlogger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "\t# Save the trained model and the tokenizer\n",
        "\t# Create output directory if needed\n",
        "\tif not os.path.exists(args.output_dir):\n",
        "\t\tos.makedirs(args.output_dir)\n",
        "\n",
        "\t# Training\n",
        "\tif args.do_train:\n",
        "\t\ttrain_ctxs = load_and_cache_ctxs(args, tokenizer, evaluate=False, pretraining=args.pretraining)\n",
        "\t\ttrain_picos = load_and_cache_picos(args, tokenizer, evaluate=False, pretraining=args.pretraining)\n",
        "\t\tglobal_step, tr_loss = train(args, train_picos, train_ctxs, model, tokenizer)\n",
        "\t\tlogger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "\t\tlogger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "\t\tmodel.save_pretrained(args.output_dir)\n",
        "\t\ttokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "\t\ttorch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "\t# Evaluation\n",
        "\tif args.do_eval:\n",
        "\t\teval_ctxs = load_and_cache_ctxs(args, tokenizer, evaluate=True)\n",
        "\t\teval_picos = load_and_cache_picos(args, tokenizer, evaluate=True)\n",
        "\n",
        "\t\tresults = {}\n",
        "\n",
        "\t\tif args.do_train: # fine-tuning at least\n",
        "\t\t\tcheckpoints = [args.output_dir]\n",
        "\t\telse: # zero-shot\n",
        "\t\t\tcheckpoints = [args.model_name_or_path]\n",
        "\n",
        "\t\tif args.eval_all_checkpoints:\n",
        "\t\t\tcheckpoints = list(\n",
        "\t\t\t\tos.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + 'full_model.bin', recursive=True))\n",
        "\t\t\t)\n",
        "\n",
        "\t\t\tlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "\n",
        "\t\tlogger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "\t\tfor checkpoint in checkpoints:\n",
        "\t\t\t# Reload the model\n",
        "\t\t\tif 'checkpoint' not in checkpoint:\n",
        "\t\t\t\tglobal_step = 'final'\n",
        "\t\t\telse:\n",
        "\t\t\t\tglobal_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "\n",
        "\t\t\tmodel = models.EBM_Net(args, path=checkpoint)\n",
        "\t\t\tmodel.to(args.device)\n",
        "\t\t\t# Evaluate\n",
        "\n",
        "\t\t\tresult = evaluate(args, eval_picos, eval_ctxs, model, tokenizer, prefix=global_step)\n",
        "\n",
        "\t\t\tresult = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "\t\t\tresults.update(result)\n",
        "\n",
        "\t\t\tif 'checkpoint' in checkpoint and args.do_train and args.eval_all_checkpoints: # eval all setting\n",
        "\t\t\t\tos.remove(os.path.join(checkpoint, 'full_model.bin'))\n",
        "\t\t\t\tos.remove(os.path.join(checkpoint, 'pytorch_model.bin'))\n",
        "\n",
        "\t\tlogger.info(\"Results: {}\".format(results))\n",
        "\t\twith open(os.path.join(args.output_dir, 'results.json'), 'w') as f:\n",
        "\t\t\tresults = {k: float(v) for k, v in results.items()}\n",
        "\t\t\tjson.dump(results, f, indent=4)\n",
        "\n",
        "\tif args.do_repr:\n",
        "\t\tlogger.info(\"Representing...\")\n",
        "\n",
        "\t\tmodel = models.EBM_Net(args, path=args.model_name_or_path)\n",
        "\t\tmodel.to(args.device)\n",
        "\n",
        "\t\trepresent(args, model, tokenizer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tmain()"
      ],
      "metadata": {
        "id": "L_DYYXTvSCmP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}